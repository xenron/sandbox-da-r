var.fevd <- fevd(model.var)
plot(var.fevd)
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-10')
rm(list=ls())
# (2) load packages
library('RODBC')                              # for reading EXCEL file
library(urca)                                 # for unit root and cointegration tests for time series data
library(tseries)                              # for time series
library(xts)                                  # for time series
library(vars)                                 # for VAR
# 1. read data
X <- odbcConnectExcel('Data-10-01.xls')
Y <- sqlFetch (X,'Sheet2')
Y <- as.xts(Y[,-1], order.by=Y[,1])
LNY <- log(Y)
colnames(LNY) <- c('LNLD', 'LNND', 'LNSH', 'LNRJ')
matplot(LNY, type='l', lty=1:ncol(LNY), col=1:ncol(LNY), ylim=c(7.0, 10.5))
legend('topleft', legend=colnames(LNY), lty=1:ncol(LNY), col=1:ncol(LNY))
# 2. do unit root test
# (1) The level sequence
ADF.LEVEL <- PP.LEVEL <- KPSS.LEVEL <- matrix(NA, nrow=2, ncol=ncol(LNY))
for (j in 1:ncol(LNY)){
ADF.LEVEL[,j] <- c(adf.test(LNY[,j])$stat, adf.test(LNY[,j])$p.value)
PP.LEVEL[,j] <- c(pp.test(LNY[,j])$stat, pp.test(LNY[,j])$p.value)
KPSS.LEVEL[,j] <- c(kpss.test(LNY[,j])$stat, kpss.test(LNY[,j])$p.value)
}
colnames(ADF.LEVEL) <- colnames(LNY); rownames(ADF.LEVEL) <- c('stat', 'p.value')
colnames(PP.LEVEL) <- colnames(LNY); rownames(PP.LEVEL) <- c('stat', 'p.value')
colnames(KPSS.LEVEL) <- colnames(LNY); rownames(KPSS.LEVEL) <- c('stat', 'p.value')
print(ADF.LEVEL); print(PP.LEVEL); print(KPSS.LEVEL)
# (2) The difference sequence
DLNY <- na.omit(diff(LNY))
ADF.DIFF <- PP.DIFF <- KPSS.DIFF <- matrix(NA, nrow=2, ncol=ncol(LNY))
for (j in 1:ncol(DLNY)){
ADF.DIFF[,j] <- c(adf.test(DLNY[,j])$stat, adf.test(DLNY[,j])$p.value)
PP.DIFF[,j] <- c(pp.test(DLNY[,j])$stat, pp.test(DLNY[,j])$p.value)
KPSS.DIFF[,j] <- c(kpss.test(DLNY[,j])$stat, kpss.test(DLNY[,j])$p.value)
}
colnames(ADF.DIFF) <- colnames(DLNY); rownames(ADF.DIFF) <- c('stat', 'p.value')
colnames(PP.DIFF) <- colnames(DLNY); rownames(PP.DIFF) <- c('stat', 'p.value')
colnames(KPSS.DIFF) <- colnames(DLNY); rownames(KPSS.DIFF) <- c('stat', 'p.value')
print(ADF.DIFF); print(PP.DIFF); print(KPSS.DIFF)
# 3. do Johansen cointegration test
model.eigen <- ca.jo(LNY, type="eigen", ecdet="none", spec="longrun")      # based on maximal eigenvalue statistic
summary(model.eigen)
# 0. initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-10')
rm(list=ls())
# (2) load packages
library('RODBC')                              # for reading EXCEL file
library(urca)                                 # for unit root and cointegration tests for time series data
library(tseries)                              # for time series
library(xts)                                  # for time series
library(vars)                                 # for VAR
# 1. read data
X <- odbcConnectExcel('Data-10-01.xls')
Y <- sqlFetch (X,'Sheet2')
Y <- as.xts(Y[,-1], order.by=Y[,1])
LNY <- log(Y)
colnames(LNY) <- c('LNLD', 'LNND', 'LNSH', 'LNRJ')
matplot(LNY, type='l', lty=1:ncol(LNY), col=1:ncol(LNY), ylim=c(7.0, 10.5))
legend('topleft', legend=colnames(LNY), lty=1:ncol(LNY), col=1:ncol(LNY))
# 1. read data
X <- odbcConnectExcel('Data-10-01.xls')
Y <- sqlFetch (X,'Sheet2')
Y <- as.xts(Y[,-1], order.by=Y[,1])
LNY <- log(Y)
colnames(LNY) <- c('LNLD', 'LNND', 'LNSH', 'LNRJ')
matplot(LNY, type='l', lty=1:ncol(LNY), col=1:ncol(LNY), ylim=c(7.0, 10.5), ylab='对数价格')
legend('topleft', legend=colnames(LNY), lty=1:ncol(LNY), col=1:ncol(LNY))
# 2. do unit root test
# (1) The level sequence
ADF.LEVEL <- PP.LEVEL <- KPSS.LEVEL <- matrix(NA, nrow=2, ncol=ncol(LNY))
for (j in 1:ncol(LNY)){
ADF.LEVEL[,j] <- c(adf.test(LNY[,j])$stat, adf.test(LNY[,j])$p.value)
PP.LEVEL[,j] <- c(pp.test(LNY[,j])$stat, pp.test(LNY[,j])$p.value)
KPSS.LEVEL[,j] <- c(kpss.test(LNY[,j])$stat, kpss.test(LNY[,j])$p.value)
}
colnames(ADF.LEVEL) <- colnames(LNY); rownames(ADF.LEVEL) <- c('stat', 'p.value')
colnames(PP.LEVEL) <- colnames(LNY); rownames(PP.LEVEL) <- c('stat', 'p.value')
colnames(KPSS.LEVEL) <- colnames(LNY); rownames(KPSS.LEVEL) <- c('stat', 'p.value')
print(ADF.LEVEL); print(PP.LEVEL); print(KPSS.LEVEL)
# (2) The difference sequence
DLNY <- na.omit(diff(LNY))
ADF.DIFF <- PP.DIFF <- KPSS.DIFF <- matrix(NA, nrow=2, ncol=ncol(LNY))
for (j in 1:ncol(DLNY)){
ADF.DIFF[,j] <- c(adf.test(DLNY[,j])$stat, adf.test(DLNY[,j])$p.value)
PP.DIFF[,j] <- c(pp.test(DLNY[,j])$stat, pp.test(DLNY[,j])$p.value)
KPSS.DIFF[,j] <- c(kpss.test(DLNY[,j])$stat, kpss.test(DLNY[,j])$p.value)
}
colnames(ADF.DIFF) <- colnames(DLNY); rownames(ADF.DIFF) <- c('stat', 'p.value')
colnames(PP.DIFF) <- colnames(DLNY); rownames(PP.DIFF) <- c('stat', 'p.value')
colnames(KPSS.DIFF) <- colnames(DLNY); rownames(KPSS.DIFF) <- c('stat', 'p.value')
print(ADF.DIFF); print(PP.DIFF); print(KPSS.DIFF)
# 3. do Johansen cointegration test
model.eigen <- ca.jo(LNY, type="eigen", ecdet="none", spec="longrun")      # based on maximal eigenvalue statistic
summary(model.eigen)
model.trace <- ca.jo(LNY, type="trace",ecdet="none", spec="longrun")           # based on maximal trace statistic
summary(model.trace)
# 4. do Johansen cointegration test with structural shift
model.break <- cajolst(LNY)
summary(model.break)
model.break@bp
# 0. 初始化
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 1. 加载包
library(fGarch)
# 2. 模拟一元ARCH时间序列模型
# (1) simulate ARCH(1) model
set.seed(12345)
spec_1 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0))
simdata_1 <- garchSim(spec_1, n=200, extended=TRUE)
class(simdata_1)
plot(simdata_1)
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) residual series')
acf(simdata_1$garch, main='(b) simulate data')
acf(simdata_1$garch^2, main='(c) squared simulate data')
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) 残差序列')
acf(simdata_1$garch, main='(b) 模拟数据')
acf(simdata_1$garch^2, main='(c) 模拟数据平方')
par(mfrow=c(1,1))
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) 残差序列', xlab='滞后期')
acf(simdata_1$garch, main='(b) 模拟数据')
acf(simdata_1$garch^2, main='(c) 模拟数据平方')
par(mfrow=c(1,1))
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) 残差序列', xlab='滞后期')
acf(simdata_1$garch, main='(b) 模拟数据', xlab='滞后期')
acf(simdata_1$garch^2, main='(c) 模拟数据平方', xlab='滞后期')
par(mfrow=c(1,1))
# (2) ARCH effect test
library(FinTS)
ArchTest(x=simdata_1$garch, lags=12)
ArchTest(x=simdata_1$eps, lags=12)
# 3. 模拟一元GARCH时间序列模型
# (1) simulate GARCH(1,1) model
set.seed(12345)
spec_2 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0.1))
simdata_2 <- garchSim(spec_2, n=200, extended=TRUE)
class(simdata_2)
par(mfrow=c(1,3))
plot(c(simdata_2$eps), type='l', xlab='day', ylab='', main='(a) residual series')
plot(c(simdata_2$garch), type='l', xlab='day', ylab='', main='(b) simulate data')
plot(c(simdata_2$sigma), type='l', xlab='day', ylab='', main='(c) conditional std dev')
par(mfrow=c(1,3))
plot(c(simdata_2$eps), type='l', xlab='天', ylab='', main='(a) 残差序列')
plot(c(simdata_2$garch), type='l', xlab='天', ylab='', main='(b) 模拟数据')
plot(c(simdata_2$sigma), type='l', xlab='天', ylab='', main='(c) 条件标准差')
par(mfrow=c(1,1))
par(mfrow=c(1,3))
acf(simdata_2$eps, xlab='滞后期', main='(a) 残差序列')
acf(simdata_2$garch, xlab='滞后期', main='(b) 模拟数据')
acf(simdata_2$garch^2, xlab='滞后期', main='(c) 模拟数据平方')
par(mfrow=c(1,1))
# 0. Initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-11')
rm(list=ls())
# 0. 初始化
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 0. initializing
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 1. read data
# (1) get price
library(quantmod)
getSymbols('^SSEC', from='2008-01-01', to='2013-12-31')           # downloads daily price data of APPLE,please wait
dim(SSEC)                                                         # dimensions of the data
names(SSEC)                                                       # names of the data
detach(package:quantmod)
# (2) comput return of stocks
price.SSEC <- SSEC$SSEC.Adjusted
r.SSEC <- 100*diff(log(price.SSEC))                                # comput daily return
r <- as.numeric(r.SSEC[-1])                                        # remove NA code
# (3) logrithm square return
y <- log(r^2 + 1e-8)                                               # plus 1e-8 to grantee positive in log
sum(is.na(y))
# 2. load filter function
source('sub-06.R')
# 3. SV modeling
# (1) Initial Parameters
phi0<-0; phi1<-.95; sQ<-.2; alpha <- mean(y); mu1<- 0; sR1<-2; mu2<- 0; sR2<-2
init.par <- c(phi0, phi1, sQ, alpha, mu1, sR1, mu2, sR2)
# (2) Innovations Likelihood
# see sub-06.R
# (3) Estimation
est <- optim(init.par, Linn, y=y, method="BFGS", hessian=TRUE, control=list(trace=1,REPORT=1))
SE <- sqrt(diag(solve(est$hessian)))
t_test <- est$par/SE
prob <- 2*pt(abs(t_test), df=length(y)-1, lower.tail=FALSE)
u <- round(cbind(estimates=est$par, SE, t_test, prob), digits=4)
rownames(u)<-c("phi0","phi1","sQ","alpha","mu1","sigv1","mu2","sigv2")
print(u)
# 4. compare distribution
# (1) filters at the estimated parameters
phi0<-est$par[1]; phi1<-est$par[2]; sQ<-est$par[3]; alpha<-est$par[4];
mu1<-est$par[5]; sR1<-est$par[6]; mu2<-est$par[7]; sR2<-est$par[8]
sv <- SVfilter(y, phi0, phi1, sQ, alpha, mu1, sR1, mu2, sR2)
# (2) densities plot (f is chi-sq, fm is fitted mixture)
x <- seq(-15, 6, by=.01)
f <- exp(-.5*(exp(x)-x))/(sqrt(2*pi))
f1 <- exp(-.5*(x-mu1)^2/sR1^2)/(sR1*sqrt(2*pi))
f2 <- exp(-.5*(x-mu2)^2/sR2^2)/(sR2*sqrt(2*pi))
fm <- (f1+f2)/2
plot(density(y),  ylim=c(0, 0.25), main='', xlab='x', ylab='density', lty=1, lwd=2, col='black')
lines(x, f, lty=2, lwd=2, col='blue')
lines(x, fm, lty=3,lwd=1, col='red')
legend('topleft', legend=c('kernel density of y', 'log of chisq', 'fitted mixture norm'),
lty=c(1,2,3), lwd=c(2,1,1), col=c('black', 'blue','red'))
plot(density(y),  ylim=c(0, 0.25), main='', xlab='x', ylab='密度', lty=1, lwd=2, col='black')
lines(x, f, lty=2, lwd=2, col='blue')
lines(x, fm, lty=3,lwd=1, col='red')
legend('topleft', legend=c('y的核密度', '卡方自然对数', '拟合的混合正态'),
lty=c(1,2,3), lwd=c(2,1,1), col=c('black', 'blue','red'))
# 5. volatility plot
Time <- 1:100
par(mfrow=c(3,1))
plot(Time, y[Time], type='l', main='SSEC平方收益自然对数)', xlab='时间', ylab='y')
plot(Time, sv$xp[Time], type='l', main='预测波动的自然对数',
ylim=c(-3.0, 2.5), xlab='时间', ylab='对数波动')
lines(Time, sv$xp[Time]+2*sqrt(sv$Pp[Time]), lty='dashed')
lines(Time, sv$xp[Time]-2*sqrt(sv$Pp[Time]), lty='dashed')
plot(Time, exp(sv$xp[Time]), type='l', main='预测波动', xlab='时间', ylab='波动率')
par(mfrow=c(1,1))
# 5. volatility plot
Time <- 1:100
par(mfrow=c(3,1))
plot(Time, y[Time], type='l', main='SSEC平方收益自然对数)', xlab='时间', ylab='y')
plot(Time, sv$xp[Time], type='l', main='预测波动的自然对数',
ylim=c(-3.0, 2.5), xlab='时间', ylab='对数波动')
lines(Time, sv$xp[Time]+2*sqrt(sv$Pp[Time]), lty='dashed')
lines(Time, sv$xp[Time]-2*sqrt(sv$Pp[Time]), lty='dashed')
plot(Time, exp(sv$xp[Time]), type='l', main='预测波动', xlab='时间', ylab='波动率')
par(mfrow=c(1,1))
# 0. initializing
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 1. load package
library(stochvol)
# 2. numerical simualtion for univariate SV process
# (1) simulate a highly persistent SV process
set.seed(12345)
sim <- svsim(500, mu = -5, phi = 0.95, sigma = 0.25)
# (2) obtain 5000 draws from the sampler
draws <- svsample(sim$y, draws=5000, burnin=500, priormu=c(-8, 1),priorphi=c(10, 1.2), priorsigma=0.2)
# (3) show estimats
print(sim)
summary(draws)
# (4) predict 20 days ahead
fore <- predict(draws, 20)
plot(draws, forecast = fore)
# plot(draws, pages=1, all.terms=TRUE, forecast = fore)
volplot(draws, forecast = 10)
volplot(draws, forecast = 10)
# 0. initializing
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 1. read data
caterp <- read.table('taq-cat-t-jan04t082010.txt', header=T)
head(caterp)
(TT <- nrow(caterp))                                      # sample size
# 2. comput price change and durations
source('sub-06.R')
hf.series <- compRtnDura(da=caterp, plot.it=TRUE)
print(hf.series$len.total)
print(hf.series$len.norm)
print(hf.series$len.duration)
print(hf.series$ratio)
layout(matrix(c(1,2,3,3), 2, 2, byrow=FALSE), c(3,2), c(2,2), respect=TRUE)
layout.show(3)
par(mar=c(4, 4, 2, 2))
plot(hf.series$prs, ylab='price', type='l')
plot(hf.series$prc, ylab='price chg', type='l')
hist(hf.series$prc, main='histogram of price chg')
# 3. extract trading information
extrTrad(da=caterp, date='2010-01-04', from='09:30', to='09:45', plot.ACF=TRUE)
# 4. comput returns
# (1) comput return series for different frequency
ints <- c(1:5, 10, 15, 20, 25, 30)                        # include 10 intervals
rtns <- list()
ntrads <- list()
for (i in seq_along(ints)){
cat('the current interval is', ints[i], '\n')
tmp <- hfanal(da=caterp, int=ints[i], basic=1)
rtns[[i]] <- tmp$returns
ntrads[[i]] <- tmp$ntrad
}
# (2) comput descriptive statistics for 1-min return series
par(mfrow=c(3,1))
par(mar=c(5.5, 5.5, 1.5, 2.5))
plot(rtns[[1]], ylab='log return', type='l')
hist(rtns[[1]], ylab='log return', main='')
acf(rtns[[1]], lag.max=300, main='')
par(mfrow=c(1,1))
# # 5. comput realized volatility
# rv <- NULL
# for (i in seq_along(ints)){
#   cat('the current interval is', ints[i], '\n')
#   tmp <- hfanal(da=caterp, int=ints[i], basic=1)
#   rv <- cbind(rv, tmp$realized)
# }
# colnames(rv) <- paste('int=', ints, sep='')
# matplot(1:nrow(rv), rv, xlab='time', ylab='realized volatility', type='l')
# legend('topleft', legend=paste('int=', ints, sep=''), lty=1)
# 6. implement ACD modeling
# (1) prepare data
sec <- 3600*caterp$hour+60*caterp$minute+caterp$second # time in seconds
ist <- 3600*9+30*60;  end <- 3600*16
lunch <- 3600*12
idx <- c(1:length(sec))[sec < ist]         # before market opens
jdx <- c(1:length(sec))[sec > end]         # after market closes
sec.norm <- sec[-c(idx,jdx)]               # normal trading hours only.
dt <- diff(sec.norm)
kdx <- c(1:length(dt))[dt > 0]             # Positive durations only
ti <- sec.norm[2:length(sec.norm)]
dt <- dt[kdx]
ti <- ti[kdx]
st <- 3600*6.5
f1 <- (ti-lunch)/st                        # trading time
# (2) fit a simple time series model
m1 <- lm(log(dt)~f1+I(f1^2))
summary(m1)
names(m1)
fit <- m1$fitted.values
adjdt <- dt/exp(fit)
plot(adjdt[1:1200], type='l', ylab='adj duration')
# 0. initializing
setwd('F:/programe/book/An Introduction to Analysis of Financial Data with R/CH-06')
rm(list=ls())
# 1. clean HF data
library(highfrequency)
data(sample_tdataraw)
dim(sample_tdataraw)
head(sample_tdataraw)
args(tradesCleanup)
tdata_afterfirstcleaning <- tradesCleanup(tdataraw=sample_tdataraw, exchanges="N")
names(tdata_afterfirstcleaning)
print(tdata_afterfirstcleaning$report)
barplot(tdata_afterfirstcleaning$report)
dim(tdata_afterfirstcleaning$tdata)
# 2. aggregate HF data
# (1) load sample price data
data("sample_tdata")
ts <- sample_tdata$PRICE
head(ts)
# (2) aggregate previous tick to the 30-second sampling frequency
args(aggregatets)
tsagg30sec = aggregatets(ts, on="seconds", k=30)
head(tsagg30sec, 20)
# (3) aggregate previous tick to the 5-minute sampling frequency
tsagg5min <- aggregatets(ts, on="minutes", k=5)
head(tsagg5min, 20)
# 3. comput descriptive statistics for realized returns
source('sub-06.R')
data(sample_real5minprices)
pdata <- sample_real5minprices[substr(time(sample_real5minprices), 12, 19)!='00:00:00']
length(pdata)
mins <- c(5, 10, 15, 20, 25, 30)
rtn <- list()
stats <- matrix(NA, nrow=length(mins), ncol=7)
for (i in seq_along(mins)){
rtn[[i]] <- HFrtn(pdata=pdata, on="minutes", k=mins[i])
stats[i,] <- stat(as.vector(rtn[[i]]))
}
head(rtn[[1]])
head(rtn[[2]])
rownames(stats) <- paste('min=', mins, sep='')
colnames(stats) <- c('mean', 'sd', 'skew', 'kurt', 'median', 'min', 'max')
print(stats)
# 0. initializing
setwd('F:/programe/book/An Introduction to Analysis of Financial Data with R/CH-06')
rm(list=ls())
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-06')
rm(list=ls())
# 1. clean HF data
library(highfrequency)
data(sample_tdataraw)
dim(sample_tdataraw)
head(sample_tdataraw)
args(tradesCleanup)
tdata_afterfirstcleaning <- tradesCleanup(tdataraw=sample_tdataraw, exchanges="N")
names(tdata_afterfirstcleaning)
print(tdata_afterfirstcleaning$report)
barplot(tdata_afterfirstcleaning$report)
dim(tdata_afterfirstcleaning$tdata)
# 2. aggregate HF data
# (1) load sample price data
data("sample_tdata")
ts <- sample_tdata$PRICE
head(ts)
# (2) aggregate previous tick to the 30-second sampling frequency
args(aggregatets)
tsagg30sec = aggregatets(ts, on="seconds", k=30)
head(tsagg30sec, 20)
# (3) aggregate previous tick to the 5-minute sampling frequency
tsagg5min <- aggregatets(ts, on="minutes", k=5)
head(tsagg5min, 20)
# 3. comput descriptive statistics for realized returns
source('sub-06.R')
data(sample_real5minprices)
pdata <- sample_real5minprices[substr(time(sample_real5minprices), 12, 19)!='00:00:00']
length(pdata)
mins <- c(5, 10, 15, 20, 25, 30)
rtn <- list()
stats <- matrix(NA, nrow=length(mins), ncol=7)
for (i in seq_along(mins)){
rtn[[i]] <- HFrtn(pdata=pdata, on="minutes", k=mins[i])
stats[i,] <- stat(as.vector(rtn[[i]]))
}
head(rtn[[1]])
head(rtn[[2]])
rownames(stats) <- paste('min=', mins, sep='')
colnames(stats) <- c('mean', 'sd', 'skew', 'kurt', 'median', 'min', 'max')
print(stats)
par(mfrow=c(2,2))
par(mar=c(4,4,2,2))
plot(mins, stats[,'mean'], xlab='mins', ylab='mean', type='o', pch='*')
plot(mins, stats[,'sd'], xlab='mins', ylab='sd', type='o', pch='*')
plot(mins, stats[,'skew'], xlab='mins', ylab='skew', type='o', pch='*')
plot(mins, stats[,'kurt'], xlab='mins', ylab='kurt', type='o', pch='*')
par(mfrow=c(1,1))
par(mfrow=c(2,2))
par(mar=c(4,4,2,2))
plot(mins, stats[,'mean'], xlab='分钟', ylab='均值', type='o', pch='*')
plot(mins, stats[,'sd'], xlab='分钟', ylab='标准差', type='o', pch='*')
plot(mins, stats[,'skew'], xlab='分钟', ylab='偏度', type='o', pch='*')
plot(mins, stats[,'kurt'], xlab='分钟', ylab='峰度', type='o', pch='*')
par(mfrow=c(1,1))
# 4. comput (weighted) realized volatility
days <- levels(factor(substr(index(pdata), 1, 10)))
rv <- matrix(NA, nrow=length(days), ncol=length(mins))
wrv <- matrix(NA, nrow=length(days), ncol=length(mins))
for (i in seq_along(mins)){
ind.day <- substr(index(rtn[[i]]), 1, 10)
rv[,i] <- aggregate(rtn[[i]], ind.day, sumsq)
ind.time <- substr(index(rtn[[i]]), 12, 19)
lambda <- aggregate(rtn[[i]], ind.time, sumsq)/sum(rtn[[i]]^2)
ind.time <- index(lambda)[lambda!=0]
ind <- substr(index(rtn[[i]]), 12, 19) %in% ind.time
rtn.f <- rtn[[i]][ind]
ind.day <- substr(index(rtn.f), 1, 10)
N <- length(ind.time)
lambda <- as.numeric(lambda)[lambda!=0]
weight <- 1/(N*lambda)
weight <- weight/sum(weight)*N
wrv[,i] <- aggregate(rtn.f, ind.day, wsumsq, w=weight)
}
rownames(rv) <- days
colnames(rv) <- paste('min=', mins, sep='')
matplot(1:nrow(rv), sqrt(rv), xlab='time', ylab='realized volatility', type='l')
legend('topright', legend=paste('min=', mins, sep=''), lty=1:6)
rownames(rv) <- days
colnames(rv) <- paste('min=', mins, sep='')
matplot(1:nrow(rv), sqrt(rv), xlab='时间', ylab='已实现波动', type='l')
legend('topright', legend=paste('min=', mins, sep=''), lty=1:6)
rownames(rv) <- days
colnames(rv) <- paste('min=', mins, sep='')
matplot(1:nrow(rv), sqrt(rv), xlab='时间', ylab='已实现波动', type='l')
legend('topright', legend=paste('min=', mins, sep=''), lty=1:6)
rownames(wrv) <- days
colnames(wrv) <- paste('min=', mins, sep='')
matplot(1:nrow(wrv), sqrt(wrv), xlab='时间', ylab='加权已实现波动', type='l')
legend('topright', legend=paste('min=', mins, sep=''), lty=1:6)
# 5. modeling realized volatility
# (1) HARRV model
data(realized_library)                                               # Get sample daily Realized Volatility data
DJI_RV <- realized_library$Dow.Jones.Industrials.Realized.Variance;  # Select DJI
DJI_RV <- DJI_RV[!is.na(DJI_RV)]                                     # Remove NA's
DJI_RV <- DJI_RV['2008']
args(harModel)
HARRV <- harModel(data=DJI_RV, periods=c(1,5,22), RVest=c("rCov"), type="HARRV",h=1)
class(HARRV)
summary(HARRV)
plot(HARRV)
matplot(predict(HARRV, interval='confidence'), type='l', xlab='index', ylab='realized volatility')
data(realized_library)                                               # Get sample daily Realized Volatility data
DJI_RV <- realized_library$Dow.Jones.Industrials.Realized.Variance;  # Select DJI
DJI_RV <- DJI_RV[!is.na(DJI_RV)]                                     # Remove NA's
DJI_RV <- DJI_RV['2008']
args(harModel)
HARRV <- harModel(data=DJI_RV, periods=c(1,5,22), RVest=c("rCov"), type="HARRV",h=1)
class(HARRV)
summary(HARRV)
plot(HARRV)
plot(HARRV, xlab='时间', ylab='波动率')
plot(HARRV, xlab='时间', ylab='波动率')
# (2) HEAVY model
returns <-  realized_library$Dow.Jones.Industrials.Returns             # realized return
rk      <-  realized_library$Dow.Jones.Industrials.Realized.Kernel     # realized measure
returns <- returns[!is.na(rk)]
rk <- rk[!is.na(rk)]                                                   # remove NA's
data <- cbind(returns^2, rk )                                          # make data matrix with squared returns and realized measures
backcast <- matrix(c(var(returns),mean(rk)), ncol=1)
args(heavyModel)
startvalues <- c(0.004,0.02,0.44,0.41,0.74,0.56)                       # initial values
HEAVY <- heavyModel(data=as.matrix(data,ncol=2), compconst=FALSE, startingvalues=startvalues, backcast=backcast)
names(HEAVY)
print(HEAVY$estparams)
plot(sqrt(HEAVY$condvar), xlab='time', ylab='volatility', main='', lty=1)
lines(sqrt(rk), lty=2)
legend('topleft', legend=c('observed realized kernel', 'HEAVY conditional volatility'),
lty=c(1,2))
plot(sqrt(HEAVY$condvar), xlab='时间', ylab='波动率', main='', lty=1)
lines(sqrt(rk), lty=2)
legend('topleft', legend=c('可观测已实现核', 'HEAVY条件波动'), lty=c(1,2))
