# Approximately equal
aeq.poismix = function(a, b, precision=c(1e-6,1e-6)) {
if( is.null(a) && is.null(b) ) return( TRUE )
if( is.null(a) || is.null(b) ) return( FALSE )
if( length(a$lambda) != length(b$lambda) ) return( FALSE )
if( max(abs(a$lambda - b$lambda) / sqrt(pmin(a$lambda, b$lambda)+1e-20))
>= precision[1] )
return( FALSE )
if( max(abs(a$pi - b$pi)) >= precision[2]) return( FALSE )
TRUE
}
# Returns a random sample of a Poisson mixture
# n        Sample size
# lambda   Means
# pi       Proportions for all components
# fixed    = TRUE, proportion of observations from each component is fixed
rpoismix = function(n=50, lambda=c(1,4), pi=1, mix, fixed=TRUE) {
if( ! missing(mix) ) {lambda =  mix$lambda; pi=mix$pi}
if( n == 0 ) return( numeric(0) )
k = length(lambda)
pi = rep(pi, len=k)
pi = pi / sum(pi)
if( fixed ) {
nj = floor(n * pi)
x = rpoismix(n - sum(nj), lambda=lambda, pi=pi, fixed=FALSE)
}
else {
cs = colSums(outer(runif(n), cumsum(pi), "<"))
nj = c(cs[1], diff(cs))
x = numeric(0)
}
for( j in 1:k ) x = c(x, rpois(nj[j], lambda[j]))
sample(x)
}
# ---------------------- #
# Distribution functions #
# ---------------------- #
# Density function of a Poisson mixture
dpoismix = function(a, x, log=FALSE) {
log.dpois = function(x, lambda) dpois(x, lambda, log=TRUE)
if (log) {
logd = outer(x, a$lambda, log.dpois)
ma = apply(logd, 1, max)
ma + log(rowSums(sweep(exp(sweep(logd, 1, ma, "-")), 2, a$pi, "*")))
}
else rowSums(sweep(outer(x, a$lambda, dpois), 2, a$pi, "*"))
}
# log-likelihood
logLik.poismix = function( a, x, w=1 ) sum( w * dpoismix(a, x, log=TRUE) )
# Line search
# mix1      Current poismix object
# mix2      New poismix object
# x         Observations
# w         Frequency
# ll.mix1   log-likelihood of mix1
# lsm       Line search method, either step-halving (halving) or optimal (optim)
line.poismix = function(mix1, mix2, x, w, ll.mix1=NULL,
lsm=c("halving","optim")) {
lsm = match.arg(lsm)
ll.alpha = function(alpha) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
logLik(m, x, w)
}
if( is.null(ll.mix1) ) ll.mix1 = logLik(mix2, x, w)
convergence = 0
alpha = 1
repeat {
if( lsm == "optim" ) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
d = m$pi - mix1$pi
gll = sum(gradient.poismix(m, x, w)$gradient * d)
if( gll < 0 ) alpha = optimize(ll.alpha, lower=0, upper=alpha, max=TRUE,
tol=1e-3)$maximum
}
ll.mix = ll.alpha(alpha)
if( ll.mix > ll.mix1 ) break
if( alpha < 1e-10 ) {convergence = 1; break}
alpha = alpha / 2
}
list(mix = poismix((1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi),
ll.mix=ll.mix, alpha=alpha, convergence=convergence)
}
# Gradient of the log-likelihood
gradient.poismix = function(mix, x, w, individual=FALSE,
hessian=FALSE) {
dij = outer(x, mix$lambda, dpois)
di = as.vector(dij %*% mix$pi)
gradient.i = w * dij / di
if( individual ) list( gradient = gradient.i )
else list( gradient = colSums(gradient.i) )
}
mix.efron = normmix(mu=c(-10.9, -7.0, -4.9, -1.8, -1.1, 0.0, 2.4, 6.1),
pi=c(1.5, 1.3, 5.6, 12.3, 13.6, 60.8, 2.7, 2.2))
set.seed(1)
x = rnormmix(n=1000, mix=mix.efron)
cnm.normmix(x, tol=1e-5)
thai = data.frame( x=c(0:21,23,24),
freq=c(120,64,69,72,54,35,36,25,25,19,18,18,13,4,3,6,6,5,1,3,1,2,1,2) )
cnm.poismix(x=thai$x, w=thai$freq)
nnls
library(quantreg)
?rq
rq
rq.fit
rq.fit.fnb
cnm.normmix(x, tol=1e-5)
cnm.poismix(x=thai$x, w=thai$freq)
install.packages('ctm')
install.packages("ctm", repos="http://R-Forge.R-project.org")
library(ctm)
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
library(ctm)
library(ctm)
library(ctm)
help(package='ctm')
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
# 0. 初始化
setwd('J:/programe/book/R with application to financial quantitive analysis/CH-05')
rm(list=ls())
# 1. 加载包
library(fGarch)
# 2. 模拟一元ARCH时间序列模型
# (1) simulate ARCH(1) model
set.seed(12345)
spec_1 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0))
simdata_1 <- garchSim(spec_1, n=200, extended=TRUE)
class(simdata_1)
plot(simdata_1)
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) residual series')
acf(simdata_1$garch, main='(b) simulate data')
acf(simdata_1$garch^2, main='(c) squared simulate data')
sumsq <- function(x){
y <- sum(x^2)
y
}
xx <- 1:5
sumsq(xx)
?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
names(lm.D9)
sumsq <- function(x){
y <- sum(x^2)
y
}
res <- function(y, x){
model.lm <- lm(y~x)
res <- model.lm$residuals
res
}
Q <- function(y, x){
ress <- res(y,x)
Q <- sumsq(ress)
Q
}
x <- seq(-5, 5, length=100)
y <- 5+2*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
x <- seq(-5, 5, length=100)
y <- 10+3*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
library(PerformanceAnalytics)
?CAPM.jensenAlpha
install.packages('fOption')
help(package='lmtest')
help(package='vars')
?read
?read.table
?save
?write
library(xlsx)
install.packages('xlsx')
library(xlsx)
library(xlsx)
library(xlsx)
library(ROCR)                     # for ROCR curve
install.packages('ROCR')
library(ROCR)                     # for ROCR curve
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
install.packages('RSNNS')
library(RSNNS)
library(RSNNS)
install.packages('Rcpp')
library(RSNNS)
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
dataValues= data[,1:7]
dataTargets = decodeClassLabels(data[,8])
data=splitForTrainingAndTest(dataValues, dataTargets, ratio=0.30)
data=normTrainingAndTestSet(data)
#建立rbf神经网络，并且训练
model2=rbf(data$inputsTrain,data$targetsTrain)
#预测
predictions2 = predict(model2,data$inputsTest)
#结果展示
confusionMatrix(data$targetsTest,predictions2)
?rbf
data
x <- as.matrix(data[,1:7])
dim(data)
data=read.delim("clipboard")
dat <- data
x <- as.matrix(dat[,1:7])
y <- as.matrix(dat[,8])
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
x
y
dim(x)
dim(y)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000,
initFuncParams=c(0, 1, 0, 0.01, 0.01),
learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)
inputs
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, size=3, maxit=1000)
model <- rbf(inputs, outputs, size=40, maxit=1000)
dim(x)
dim(y)
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y)
y
y
x
as.matrix(sin(inputs) + runif(inputs*0.2))
normalizeData(outputs, "0_1")
model.RBF <- rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
normalizeData(y, "0_1")
rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
inputs
outputs
outputs <- normalizeData(outputs, "0_1")
outputs
model <- rbf(inputs, outputs, size=40, maxit=1000)
inputs=x
outputs=normalizeData(y, "0_1")
inputs
class(inputs)
colnames(inputs)
colnames(inputs)<-''
colnames(inputs)<-NULL
head(inputs)
rbf(inputs, outputs)
rbf(inputs, outputs, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000)
rbf(inputs, outputs, size=40, maxit=1000)
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=3, maxit=1000)
model.RBF
par(mfrow=c(2,1))
plotIterativeError(model.RBF)
plot(inputs, outputs)
lines(inputs, fitted(model.RBF), col="green")
plotIterativeError(model.RBF)               # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
install.packages('DAAG')
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
library(DAAG)
install.packages('DAAG')
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
par(mfrow=c(1,1))
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=5, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=15, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
library(LASSO)
instll.packages('LASSO')
install.packages('LASSO')
library(lasso)
install.packages(lasso)
library(LARS)
install.packages('LARS')
install.packages('lasso')
install.packages('lars')
library(elasticnet)
install.packages('elasticnet')
install.packages('ncvreg')
help(package='ncvreg')
help(package='elasticnet')
help(package='lars')
help(package='lars')
help(package='elasticnet')
help(ncvreg)
help(package='ncvreg')
library(urca)
help(package='urca')
?cajolst
library(quantreg)
data(engel)
getwd()
write.csv(engel, file='ls.csv')
names(engel)
model.rq <- rq(foodexp~income, data=engel)
model.rq
?rq
taus <- seq(0.1,0.9, by=0.1)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
model.rq
taus <- seq(0.01,0.99, by=0.01)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
mid.income <- mean(engel$income)
mid.income
plot(engel$income)
?predict
foodfit <- predict(model.rq, newdata=data.frame(income=mid.income))
foodfit
plot(density(foodfit))
low.income <- quantile(engel$income, prob=0.25)
plot(income, foodexp, data=engel)
plot(engel$income, engel$foodexp)
??return
??returns
help(package='fPortfolio')
library(fPortfolio)
?returns
library(fPortfolioBacktest)
help(package='fPortfolioBacktest')
# 0. initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-12')
rm(list=ls())
# 0. Initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-03')
rm(list = ls())
# (2) load packages
library(lars)           # for LASSO regression
library(ridge)          # for ridge regression
library(mvtnorm)        # for multivariate normal distribution
# 1. generate data
N <- 30
B <- 50
rho <- 0.5
sigma <- 3
betas <- c(3.5,1,0,2,0,0)
set.seed(12345)
I <- matrix(rep(1:length(betas), times=length(betas)), byrow=FALSE, nrow=length(betas))
J <- matrix(rep(1:length(betas), times=length(betas)), byrow=TRUE, nrow=length(betas))
(corr <- rho^abs(I-J))
eps <- rnorm(N, mean=0, sd=1)
X <- rmvnorm(n=N, mean=rep(0, length(betas)), sigma=corr)
cor(X)
Y <- X %*% betas + sigma * eps
dat <- data.frame(Y, X)
# 2. do regression for one simulation
# (1) do OLS regression
model.ols <- lm(Y~.-1, data=dat)
summary(model.ols)
coef.ols <- coef(model.ols)
coef.ols[coef.ols!=0]                           # show those coefficients not equal to 0
# (2) do ridge regression
model.rid <- linearRidge(Y~.-1, data=dat)
summary(model.rid)
coef.rid <- coef(model.rid)
coef.rid[coef.rid!=0]                            # show those coefficients not equal to 0
# (3) do lasso regression
model.lasso <- lars(X, Y, type='lasso')          # make model
plot(model.lasso)                                # give plot
summary(model.lasso)
set.seed(12345)
CV.lasso <- cv.lars(X, Y, K=10)                  # do cross validation
(best <- CV.lasso$index[which.min(CV.lasso$cv)])   # select best value
(coef.lasso <- coef.lars(model.lasso, mode='fraction', s=best))
names(coef.lasso) <- colnames(dat)[-1]
coef.lasso[coef.lasso!=0]                        # show those coefficients not equal to 0
# 3. do regression for 50 simulations
# (1) define Monte Carlo function
MonteCarlo <- function(N, betas, type='lasso'){
# (1) generate data
I <- matrix(rep(1:length(betas), times=length(betas)), byrow=FALSE, nrow=length(betas))
J <- matrix(rep(1:length(betas), times=length(betas)), byrow=TRUE, nrow=length(betas))
corr <- rho^abs(I-J)
eps <- rnorm(N, mean=0, sd=1)
X <- rmvnorm(n=N, mean=rep(0, length(betas)), sigma=corr)
Y <- X %*% betas + sigma * eps
dat <- data.frame(Y, X)
# (2) do OLS regression
model.ols <- lm(Y~.-1, data=dat)
coef.ols <- coef(model.ols)
# (3) do ridge regression
model.rid <- linearRidge(Y~.-1, data=dat)
coef.rid <- coef(model.rid)
# (4) do lasso regression
model.lasso <- lars(X, Y, type=type)
CV.lasso <- cv.lars(X, Y, K=10, plot.it=FALSE)
best <- CV.lasso$index[which.min(CV.lasso$cv)]
coef.lasso <- coef.lars(model.lasso, mode='fraction', s=best)
names(coef.lasso) <- colnames(dat)[-1]
# (5) output
ans <- list(coef.ols=coef.ols, coef.rid=coef.rid, coef.lasso=coef.lasso)
ans
}
# (2) repeat simulation 50 times
coef.ols <- coef.rid <- coef.lasso <- matrix(NA, nrow=length(betas), ncol=B)
for (b in 1:B){
coef.MC <- MonteCarlo(N, betas, type='lasso')
coef.ols[,b] <- coef.MC$coef.ols
coef.rid[,b] <- coef.MC$coef.rid
coef.lasso[,b] <- coef.MC$coef.lasso
}
coef.methods <- matrix(NA, nrow=B, ncol=3)
for (i in 1:length(betas)){
coef.methods <- cbind(coef.ols[i,], coef.rid[i,], coef.lasso[i,])
colnames(coef.methods) <- c('OLS', 'ridge', 'LASSO')
boxplot(coef.methods, main=paste('coef of X', i, sep=''))
abline(h=betas[i], lty=3)
}
coef.methods <- matrix(NA, nrow=B, ncol=3)
for (i in 1:length(betas)){
coef.methods <- cbind(coef.ols[i,], coef.rid[i,], coef.lasso[i,])
colnames(coef.methods) <- c('OLS', 'ridge', 'LASSO')
boxplot(coef.methods, main=paste('X', i, '的系数', sep=''))
abline(h=betas[i], lty=3)
}
