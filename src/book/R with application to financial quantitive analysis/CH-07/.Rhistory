if( max(abs(a$lambda - b$lambda) / sqrt(pmin(a$lambda, b$lambda)+1e-20))
>= precision[1] )
return( FALSE )
if( max(abs(a$pi - b$pi)) >= precision[2]) return( FALSE )
TRUE
}
# Returns a random sample of a Poisson mixture
# n        Sample size
# lambda   Means
# pi       Proportions for all components
# fixed    = TRUE, proportion of observations from each component is fixed
rpoismix = function(n=50, lambda=c(1,4), pi=1, mix, fixed=TRUE) {
if( ! missing(mix) ) {lambda =  mix$lambda; pi=mix$pi}
if( n == 0 ) return( numeric(0) )
k = length(lambda)
pi = rep(pi, len=k)
pi = pi / sum(pi)
if( fixed ) {
nj = floor(n * pi)
x = rpoismix(n - sum(nj), lambda=lambda, pi=pi, fixed=FALSE)
}
else {
cs = colSums(outer(runif(n), cumsum(pi), "<"))
nj = c(cs[1], diff(cs))
x = numeric(0)
}
for( j in 1:k ) x = c(x, rpois(nj[j], lambda[j]))
sample(x)
}
# ---------------------- #
# Distribution functions #
# ---------------------- #
# Density function of a Poisson mixture
dpoismix = function(a, x, log=FALSE) {
log.dpois = function(x, lambda) dpois(x, lambda, log=TRUE)
if (log) {
logd = outer(x, a$lambda, log.dpois)
ma = apply(logd, 1, max)
ma + log(rowSums(sweep(exp(sweep(logd, 1, ma, "-")), 2, a$pi, "*")))
}
else rowSums(sweep(outer(x, a$lambda, dpois), 2, a$pi, "*"))
}
# log-likelihood
logLik.poismix = function( a, x, w=1 ) sum( w * dpoismix(a, x, log=TRUE) )
# Line search
# mix1      Current poismix object
# mix2      New poismix object
# x         Observations
# w         Frequency
# ll.mix1   log-likelihood of mix1
# lsm       Line search method, either step-halving (halving) or optimal (optim)
line.poismix = function(mix1, mix2, x, w, ll.mix1=NULL,
lsm=c("halving","optim")) {
lsm = match.arg(lsm)
ll.alpha = function(alpha) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
logLik(m, x, w)
}
if( is.null(ll.mix1) ) ll.mix1 = logLik(mix2, x, w)
convergence = 0
alpha = 1
repeat {
if( lsm == "optim" ) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
d = m$pi - mix1$pi
gll = sum(gradient.poismix(m, x, w)$gradient * d)
if( gll < 0 ) alpha = optimize(ll.alpha, lower=0, upper=alpha, max=TRUE,
tol=1e-3)$maximum
}
ll.mix = ll.alpha(alpha)
if( ll.mix > ll.mix1 ) break
if( alpha < 1e-10 ) {convergence = 1; break}
alpha = alpha / 2
}
list(mix = poismix((1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi),
ll.mix=ll.mix, alpha=alpha, convergence=convergence)
}
# Gradient of the log-likelihood
gradient.poismix = function(mix, x, w, individual=FALSE,
hessian=FALSE) {
dij = outer(x, mix$lambda, dpois)
di = as.vector(dij %*% mix$pi)
gradient.i = w * dij / di
if( individual ) list( gradient = gradient.i )
else list( gradient = colSums(gradient.i) )
}
mix.efron = normmix(mu=c(-10.9, -7.0, -4.9, -1.8, -1.1, 0.0, 2.4, 6.1),
pi=c(1.5, 1.3, 5.6, 12.3, 13.6, 60.8, 2.7, 2.2))
set.seed(1)
x = rnormmix(n=1000, mix=mix.efron)
cnm.normmix(x, tol=1e-5)
thai = data.frame( x=c(0:21,23,24),
freq=c(120,64,69,72,54,35,36,25,25,19,18,18,13,4,3,6,6,5,1,3,1,2,1,2) )
cnm.poismix(x=thai$x, w=thai$freq)
nnls
library(quantreg)
?rq
rq
rq.fit
rq.fit.fnb
cnm.normmix(x, tol=1e-5)
cnm.poismix(x=thai$x, w=thai$freq)
install.packages('ctm')
install.packages("ctm", repos="http://R-Forge.R-project.org")
library(ctm)
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
library(ctm)
library(ctm)
library(ctm)
help(package='ctm')
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
# 0. 初始化
setwd('J:/programe/book/R with application to financial quantitive analysis/CH-05')
rm(list=ls())
# 1. 加载包
library(fGarch)
# 2. 模拟一元ARCH时间序列模型
# (1) simulate ARCH(1) model
set.seed(12345)
spec_1 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0))
simdata_1 <- garchSim(spec_1, n=200, extended=TRUE)
class(simdata_1)
plot(simdata_1)
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) residual series')
acf(simdata_1$garch, main='(b) simulate data')
acf(simdata_1$garch^2, main='(c) squared simulate data')
sumsq <- function(x){
y <- sum(x^2)
y
}
xx <- 1:5
sumsq(xx)
?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
names(lm.D9)
sumsq <- function(x){
y <- sum(x^2)
y
}
res <- function(y, x){
model.lm <- lm(y~x)
res <- model.lm$residuals
res
}
Q <- function(y, x){
ress <- res(y,x)
Q <- sumsq(ress)
Q
}
x <- seq(-5, 5, length=100)
y <- 5+2*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
x <- seq(-5, 5, length=100)
y <- 10+3*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
library(PerformanceAnalytics)
?CAPM.jensenAlpha
install.packages('fOption')
help(package='lmtest')
help(package='vars')
?read
?read.table
?save
?write
library(xlsx)
install.packages('xlsx')
library(xlsx)
library(xlsx)
library(xlsx)
library(ROCR)                     # for ROCR curve
install.packages('ROCR')
library(ROCR)                     # for ROCR curve
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
install.packages('RSNNS')
library(RSNNS)
library(RSNNS)
install.packages('Rcpp')
library(RSNNS)
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
dataValues= data[,1:7]
dataTargets = decodeClassLabels(data[,8])
data=splitForTrainingAndTest(dataValues, dataTargets, ratio=0.30)
data=normTrainingAndTestSet(data)
#建立rbf神经网络，并且训练
model2=rbf(data$inputsTrain,data$targetsTrain)
#预测
predictions2 = predict(model2,data$inputsTest)
#结果展示
confusionMatrix(data$targetsTest,predictions2)
?rbf
data
x <- as.matrix(data[,1:7])
dim(data)
data=read.delim("clipboard")
dat <- data
x <- as.matrix(dat[,1:7])
y <- as.matrix(dat[,8])
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
x
y
dim(x)
dim(y)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000,
initFuncParams=c(0, 1, 0, 0.01, 0.01),
learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)
inputs
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, size=3, maxit=1000)
model <- rbf(inputs, outputs, size=40, maxit=1000)
dim(x)
dim(y)
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y)
y
y
x
as.matrix(sin(inputs) + runif(inputs*0.2))
normalizeData(outputs, "0_1")
model.RBF <- rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
normalizeData(y, "0_1")
rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
inputs
outputs
outputs <- normalizeData(outputs, "0_1")
outputs
model <- rbf(inputs, outputs, size=40, maxit=1000)
inputs=x
outputs=normalizeData(y, "0_1")
inputs
class(inputs)
colnames(inputs)
colnames(inputs)<-''
colnames(inputs)<-NULL
head(inputs)
rbf(inputs, outputs)
rbf(inputs, outputs, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000)
rbf(inputs, outputs, size=40, maxit=1000)
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=3, maxit=1000)
model.RBF
par(mfrow=c(2,1))
plotIterativeError(model.RBF)
plot(inputs, outputs)
lines(inputs, fitted(model.RBF), col="green")
plotIterativeError(model.RBF)               # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
install.packages('DAAG')
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
library(DAAG)
install.packages('DAAG')
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
par(mfrow=c(1,1))
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=5, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=15, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
library(LASSO)
instll.packages('LASSO')
install.packages('LASSO')
library(lasso)
install.packages(lasso)
library(LARS)
install.packages('LARS')
install.packages('lasso')
install.packages('lars')
library(elasticnet)
install.packages('elasticnet')
install.packages('ncvreg')
help(package='ncvreg')
help(package='elasticnet')
help(package='lars')
help(package='lars')
help(package='elasticnet')
help(ncvreg)
help(package='ncvreg')
library(urca)
help(package='urca')
?cajolst
library(quantreg)
data(engel)
getwd()
write.csv(engel, file='ls.csv')
names(engel)
model.rq <- rq(foodexp~income, data=engel)
model.rq
?rq
taus <- seq(0.1,0.9, by=0.1)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
model.rq
taus <- seq(0.01,0.99, by=0.01)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
mid.income <- mean(engel$income)
mid.income
plot(engel$income)
?predict
foodfit <- predict(model.rq, newdata=data.frame(income=mid.income))
foodfit
plot(density(foodfit))
low.income <- quantile(engel$income, prob=0.25)
plot(income, foodexp, data=engel)
plot(engel$income, engel$foodexp)
??return
??returns
help(package='fPortfolio')
library(fPortfolio)
?returns
library(fPortfolioBacktest)
help(package='fPortfolioBacktest')
# 0. Initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-03')
rm(list = ls())
# 0.Initializing
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-07')
rm(list=ls())
# 1. Relationship between loss and return
# (1) random data
set.seed(1)
L <- rchisq(1000, df=5)     # random generation for the chi-squared (chi^2) distribution
R <- -L
# (2) quantile
Q.L <- quantile(L, probs = c(0.05, 0.1, 0.5, 0.9, 0.95))  #produces sample quantiles corresponding to the given probabilities
Q.R <- quantile(R, probs = c(0.05, 0.1, 0.5, 0.9, 0.95))
# (3) figure
plot(c(density(L)$x, density(R)$x), c(density(L)$y, density(R)$y), xlab='收益或损失', ylab='密度', type='n')
lines(density(L), lty=1)
lines(density(R), lty=2)
legend('topleft', legend=c('损失', '收益'), lty=c(1,2))
# 2.Download financial time series data
library(quantmod)                                                 # loads package
getSymbols('AAPL', from='1989-12-01', to='2013-11-30')            # downloads daily price data of APPLE,please wait
dim(AAPL)                                                         # dimensions of daily price data
chartSeries(AAPL, theme='white')                                  # creates time series plot of price and deal
detach(package:quantmod)
price.AAPL <- AAPL$AAPL.Adjusted
R <- 100*diff(log(price.AAPL))                                    # gets daily log return series of APPLE
r <- (-1)*R[-1]
plot(r)
# 3.Calculate VaR and ES via Riskmetrics model
library(rugarch)                                               # load package
spec1 = ugarchspec(variance.model=list(model="iGARCH", garchOrder=c(1,1)),
mean.model=list(armaOrder=c(0,0), include.mean=FALSE),
distribution.model="norm", fixed.pars=list(omega=0,alpha1=0.06))
# method for creating a univariate GARCH specification object prior to fitting
forecast1 <- ugarchforecast(spec1, data=r,n.roll=0,n.ahead=1,out.sample=1)
# method for forecasting from a variety of univariate GARCH models
forecast1
VaR1 <- qnorm(0.95)*1.278
ES1 <- dnorm(qnorm(0.95))*1.278/0.05
VaR5 <- qnorm(0.95)*1.278*sqrt(5)
ES5 <- dnorm(qnorm(0.95))*1.278*sqrt(5)/0.05
# 4.Calculate VaR and ES via GARCH model
library(fGarch)
m1 <- garchFit(formula = ~arma(0,0)+garch(1,1), data = r, cond.dist = c("norm"), trace = FALSE)
# estimates the parameters of an univariate ARMA-GARCH/APARCH process
summary(m1)
p1 <- predict(m1,n.ahead=1)   #  1-step-ahead forecasts of the conditional mean and conditional variance
VaR <- p1$meanForecast+p1$standardDeviation*qnorm(0.95)
ES <- p1$meanForecast+p1$standardDeviation*dnorm(qnorm(0.95))/(1-0.95)
m2 <- garchFit(formula = ~arma(0,0)+garch(1,1), data = r, cond.dist = c("norm"), trace = FALSE)
p2 <- predict(m2,n.ahead=5)
VaR <- p2$meanForecast[5]+p2$standardDeviation[5]*qnorm(0.95)
ES <- p2$meanForecast[5]+p2$standardDeviation[5]*dnorm(qnorm(0.95))/(1-0.95)
m3 <- garchFit(formula = ~arma(0,0)+garch(1,1), data = r, cond.dist = c("std"), trace = FALSE)
summary(m3)
p3 <- predict(m3,n.ahead=1)
VaR <- p3$meanForecast+p3$standardDeviation*qt(0.95,5)/sqrt(5/3)
ES <- p3$meanForecast+p3$standardDeviation*dt(qt(0.95,5)/sqrt(5/3),5)/(1-0.95)
library(quantreg)                  # loads package
fit1 <- rq(formula = r.price ~ r.volume, tau = 0.5, data = SSEC)    # linear quantile regression fit
fit1                               # views coefficients
plot(r.volume, r.price, xlab= "交易量", ylab="价格", type = "n")
points(r.volume, r.price, col= "blue")
abline(lm( r.price ~ r.volume), lty=1, lwd=1)
abline(rq( r.price ~ r.volume), lty=2, lwd=2)
taus <- c( 0.05, 0.1, 0.25, 0.75, 0.9, 0.95)
for(i in 1:length(taus)){
abline(rq(r.price ~ r.volume, tau=taus[i]))
}
# 0.Initializing
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-07')
rm(list=ls())
# 1.Estimates Parameters of linear quantile regression
# (1) quantile regression Via simplex method
library(quantmod)                                         # loads package
getSymbols('^SSEC', from='2012-01-01', to='2013-12-31')   # downloads daily price data of SSEC,please wait,please wait
price <- as.numeric(SSEC$SSEC.Adjusted)               # gets price data of SSEC
volume <- as.numeric(SSEC$SSEC.Volume)                # gets volume data of SSEC
r.price <- diff(log(price))                           # gets daily log ruturns
r.volume <- (log(volume))[-1]                         # gets logarithmic of volume
SSEC <- data.frame(r.price,r.volume)                  # produces a dataframe
library(quantreg)                  # loads package
fit1 <- rq(formula = r.price ~ r.volume, tau = 0.5, data = SSEC)    # linear quantile regression fit
fit1                               # views coefficients
plot(r.volume, r.price, xlab= "交易量", ylab="价格", type = "n")
points(r.volume, r.price, col= "blue")
abline(lm( r.price ~ r.volume), lty=1, lwd=1)
abline(rq( r.price ~ r.volume), lty=2, lwd=2)
taus <- c( 0.05, 0.1, 0.25, 0.75, 0.9, 0.95)
for(i in 1:length(taus)){
abline(rq(r.price ~ r.volume, tau=taus[i]))
}
plot(r.volume, r.price, xlab= "交易量", ylab="价格", type = "n")
points(r.volume, r.price, col= "blue")
abline(lm( r.price ~ r.volume), lty=1, lwd=2)
abline(rq( r.price ~ r.volume), lty=2, lwd=2)
taus <- c( 0.05, 0.1, 0.25, 0.75, 0.9, 0.95)
for(i in 1:length(taus)){
abline(rq(r.price ~ r.volume, tau=taus[i]), lty=3)
}
