u^2*abs(tau - (u<0))
}
# 2. loss function
loss <- function(tau, y, xi){
sum(rho(tau, u=y-xi))
}
# 3. R-objective function, see P8.
Rfun <- function(tau, y, xi){
R <- numeric(length(xi))
for (i in 1:length(xi)){
R[i] <- loss(tau=tau, y, xi[i])
}
plot(xi, R, type='l', lwd=2, xlab=expression(xi), ylab='objective function')
for (j in 1:length(y)){
abline(v=y[j], lty=2)
#     lines(x=c(y[j], y[j]), y=c(0,R[j]), lty=2)
}
return(R)
}
# 4. loss function and objective function
# (1) plot of loss functions
u <- seq(-2, 2, length=100)
square <- u^2
rho_025 <- rho(tau=0.25, u=u)
rho_050 <- rho(tau=0.50, u=u)
rho_075 <- rho(tau=0.75, u=u)
plot(c(u,u,u,u), c(rho_025, rho_050, rho_075, square), type='n', xlab='u', ylab='loss')
lines(u, rho_050, lty=1, lwd=2, col='red')
lines(u, rho_025, lty=2, lwd=2, col='blue')
lines(u, rho_075, lty=3, lwd=2, col='blue')
lines(u, square,  lty=1, lwd=2, col='black')
legend('top', legend=c('rho_025','rho_050','rho_075','square'), lty=c(1,2,3), lwd=c(2,2,2), col=c('red','blue','blue','black'))
# (2) plot of loss functions
u <- seq(-2, 2, length=100)
rho_025 <- rho(tau=0.25, u=u)
rho_050 <- rho(tau=0.50, u=u)
rho_075 <- rho(tau=0.75, u=u)
qrho_025 <- qrho(tau=0.25, u=u)
qrho_050 <- qrho(tau=0.50, u=u)
qrho_075 <- qrho(tau=0.75, u=u)
plot(rep(u, 4), c(rho_025, rho_050, qrho_025, qrho_050), type='n', xlab='u', ylab='loss')
lines(u, qrho_025, lty=1, lwd=2, col='blue')
lines(u, qrho_050, lty=1, lwd=1, col='red')
lines(u, rho_025, lty=2, lwd=2, col='blue')
lines(u, rho_050, lty=2, lwd=1, col='red')
legend('top', legend=c(expression(rho),expression(rho),expression(rho),expression(rho)), lty=c(2,2,1,1), lwd=c('blue','red','blue','red'))
plot(rep(u, 4), c(rho_025, rho_050, qrho_025, qrho_050), las=1, type='n', xlab='u', ylab='loss')
lines(u, qrho_025, lty=1, lwd=2, col='blue')
lines(u, qrho_050, lty=1, lwd=1, col='red')
lines(u, rho_025, lty=2, lwd=2, col='blue')
lines(u, rho_050, lty=2, lwd=1, col='red')
legend('top', legend=c(expression(rho[tau==0.25]^(E)),expression(rho[tau==0.50]^(E)),expression(rho[tau==0.25]^(Q)),expression(rho[tau==0.50]^(Q))),
lty=c(1,1,2,2), lwd=c(2,1,2,1), col=c('blue','red','blue','red'))
# (2) objective function
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 1/3
(R <- Rfun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 5. simulation by Monte Carlo, linear quantile regression
# (1) Set parameters
n <- 100
name.dist <- 'chisq'               # 'normal', 't', 'chisq'
taus <- c(0.1, 0.5, 0.9)
# (2) Generate data
x <- runif(n=n, min=0, max=10)
mx <- 10 + 5*x
if (name.dist=='normal'){
epsi <- rnorm(n=n, mean=0, sd=1)
Finv <- qnorm(p=taus, mean=0, sd=1)
}
if (name.dist=='t'){
epsi <- rt(n=n, df=3)
Finv <- qt(p=taus, df=3)
}
if (name.dist=='chisq'){
epsi <- rchisq(n=n, df=3)
Finv <- qchisq(p=taus, df=3)
}
y1 <- mx + epsi
y2 <- mx + x*epsi
qy1_01 <- mx + Finv[1]
qy1_05 <- mx + Finv[2]
qy1_09 <- mx + Finv[3]
qy2_01 <- mx + x*Finv[1]
qy2_05 <- mx + x*Finv[2]
qy2_09 <- mx + x*Finv[3]
# (3) Plot of data
par(mfrow=c(1,2))
plot(c(x,x,x,x,x), c(y1, mx, qy1_01, qy1_05, qy1_09), type='n', xlab='x', ylab='y', main='iid')
points(x, y1)
lines(x, mx, lty=1, lwd=2, col='black')
lines(x, qy1_01, lty=2, lwd=1, col='blue')
lines(x, qy1_05, lty=2, lwd=2, col='red')
lines(x, qy1_09, lty=2, lwd=1, col='blue')
plot(c(x,x,x,x,x), c(y2, mx, qy2_01, qy2_05, qy2_09), type='n', xlab='x', ylab='y', main='non iid')
points(x, y2)
lines(x, mx, lty=1, lwd=2, col='black')
lines(x, qy2_01, lty=2, lwd=1, col='blue')
lines(x, qy2_05, lty=2, lwd=2, col='red')
lines(x, qy2_09, lty=2, lwd=1, col='blue')
# (4) Make model
library(quantreg)
model1 <- rq(y1~x, tau=taus)
summary(model1)
qyfit1 <- predict(model1)
model2 <- rq(y2~x, tau=taus)
summary(model2)
qyfit2 <- predict(model2)
# (5) Predict
par(mfrow=c(1,1))
plot(c(x,x,x,x,x,x,x), c(y1, qy1_01, qy1_05, qy1_09, qyfit1), type='n', xlab='x', ylab='y', main='iid')
points(x, y1)
lines(x, qy1_01, lty=1, lwd=2, col='blue')
lines(x, qy1_05, lty=1, lwd=2, col='red')
lines(x, qy1_09, lty=1, lwd=2, col='green')
lines(x, qyfit1[,1], lty=3, lwd=1, col='blue')
lines(x, qyfit1[,2], lty=3, lwd=1, col='red')
lines(x, qyfit1[,3], lty=3, lwd=1, col='green')
par(mfrow=c(1,1))
plot(c(x,x,x,x,x,x,x), c(y2, qy2_01, qy2_05, qy2_09, qyfit1), type='n', xlab='x', ylab='y', main='iid')
points(x, y2)
lines(x, qy2_01, lty=1, lwd=2, col='blue')
lines(x, qy2_05, lty=1, lwd=2, col='red')
lines(x, qy2_09, lty=1, lwd=2, col='green')
lines(x, qyfit2[,1], lty=3, lwd=1, col='blue')
lines(x, qyfit2[,2], lty=3, lwd=1, col='red')
lines(x, qyfit2[,3], lty=3, lwd=1, col='green')
# 6. simulation by Monte Carlo, nonlinear quantile regression
# (1) Define function for Simulate data by Lu Zhengbo
Data_sim_lzb <- function(n, beta, error.type){
# (1) Generate predictor: x
x <- runif(n=n, min=-4, max=4)
# (2) Generate random error: epsi
tau <- c(0.1, 0.5, 0.9)
if (tolower(error.type)=='normal'){
epsi <- rnorm(n=n, mean=0, sd=1)
Finv <- qnorm(p=tau, mean=0, sd=1)
}
if (tolower(error.type)=='t'){
epsi <- rt(n=n, df=3)
Finv <- qt(p=tau, df=3)
}
if (tolower(error.type)=='chisq'){
epsi <- rchisq(n=n, df=3)
Finv <- qchisq(p=tau, df=3)
}
# (3) Generate response variable: y
mx <- (1-x+2*x^2)*exp(-0.5*x^2)
sigx <- (1 + beta*x)/5
y <- mx + sigx*epsi
# (4) Compute real conditional quantile of y given x
qy_01 <- mx + sigx*Finv[1]       # at tau=0.1
qy_05 <- mx + sigx*Finv[2]       # at tau=0.5
qy_09 <- mx + sigx*Finv[3]       # at tau=0.9
qy <- cbind(qy_01, qy_05, qy_09)
# (4) Plot
par(mfrow=c(1,2))
plot(density(epsi), main='density plot of error')
plot(c(x, x), c(y, mx), type='n', xlab='x', ylab='y')
points(x, y, pch='.', cex=2.0)
lines(x[order(x)], mx[order(x)], lwd=2)
lines(x[order(x)], qy_01[order(x)], lty=2)
lines(x[order(x)], qy_05[order(x)], lty=2)
lines(x[order(x)], qy_09[order(x)], lty=2)
# (5) Output results
results <- list(y=y, qy=qy, mx=mx, x=x, epsi=epsi)
return(results)
}
# (2) Generate simulation data
Data_normal <- Data_sim_lzb(n=100, beta=0.2, error.type='normal')
Data_t <- Data_sim_lzb(n=100, beta=0.2, error.type='t')
Data_chisq <- Data_sim_lzb(n=100, beta=0.2, error.type='chisq')
# 7. mcycle data application, nonlinear quantile regression
# (1) real data
library(MASS)
data(mcycle)
attach(mcycle)
# (2) lprq model
library(quantreg)
par(mfrow=c(1,1))
plot(times,accel,xlab = "milliseconds", ylab = "acceleration (in g)")
hs <- c(1,2,3,4)
for(i in hs){
h = hs[i]
fit <- lprq(times,accel,h=h,tau=.5)
lines(fit$xx,fit$fv,lty=i)
}
legend('topleft', c("h=1","h=2","h=3","h=4"),lty=1:length(hs))
# 3. R-objective function, see P8.
Rfun <- function(tau, y, xi){
R <- numeric(length(xi))
for (i in 1:length(xi)){
R[i] <- loss(tau=tau, y, xi[i])
}
plot(xi, R, type='l', lwd=2, xlab=expression(xi), ylab='objective function')
for (j in 1:length(y)){
abline(v=y[j], lty=2)
#     lines(x=c(y[j], y[j]), y=c(0,R[j]), lty=2)
}
return(R)
}
# 4. loss function and objective function
# (1) plot of loss functions
u <- seq(-2, 2, length=100)
square <- u^2
rho_025 <- rho(tau=0.25, u=u)
rho_050 <- rho(tau=0.50, u=u)
rho_075 <- rho(tau=0.75, u=u)
plot(c(u,u,u,u), c(rho_025, rho_050, rho_075, square), type='n', xlab='u', ylab='loss')
lines(u, rho_050, lty=1, lwd=2, col='red')
lines(u, rho_025, lty=2, lwd=2, col='blue')
lines(u, rho_075, lty=3, lwd=2, col='blue')
lines(u, square,  lty=1, lwd=2, col='black')
legend('top', legend=c('rho_025','rho_050','rho_075','square'), lty=c(1,2,3), lwd=c(2,2,2), col=c('red','blue','blue','black'))
# (2) plot of loss functions
u <- seq(-2, 2, length=100)
rho_025 <- rho(tau=0.25, u=u)
rho_050 <- rho(tau=0.50, u=u)
rho_075 <- rho(tau=0.75, u=u)
qrho_025 <- qrho(tau=0.25, u=u)
qrho_050 <- qrho(tau=0.50, u=u)
qrho_075 <- qrho(tau=0.75, u=u)
plot(rep(u, 4), c(rho_025, rho_050, qrho_025, qrho_050), type='n', xlab='u', ylab='loss')
lines(u, qrho_025, lty=1, lwd=2, col='blue')
lines(u, qrho_050, lty=1, lwd=1, col='red')
lines(u, rho_025, lty=2, lwd=2, col='blue')
lines(u, rho_050, lty=2, lwd=1, col='red')
legend('top', legend=c(expression(rho),expression(rho),expression(rho),expression(rho)), lty=c(2,2,1,1), lwd=c('blue','red','blue','red'))
# (2) objective function
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 1/3
(R <- Rfun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
Rfun
y
R
Rfun
loss
rho
source(Sub-02.R)                 # our own functions
source('Sub-02.R')                 # our own functions
# 1. solve a nonlinear equation
# (1) define a loss function
f <- function(r, p, Cs){
n <- length(Cs)
tt <- 1:n
loss <- p - sum(Cs/((1+r)^tt))
loss
}
# (2) find the solution
Cs <- c(2000, 2000, 2500, 4000)
P <- 7704
uniroot(f, c(0,1), p=P, Cs=Cs)           # find the zero root of f function
# 2. optimize a nonlinear function
# (1) optimize
g <- function(r, p, Cs) {f(r, p, Cs)^2}   # define g function: g=f^2
optimize(g, c(0,1), p=P, Cs=Cs)           # optimize g function
# (2) compare tow approaches
rs <- seq(0, 1, length=100)
fval <- gval <- numeric(length(rs))
for (i in seq_along(rs)){
fval[i] <- f(r=rs[i], p=P, Cs=Cs)
gval[i] <- g(r=rs[i], p=P, Cs=Cs)
}
par(mfrow=c(2,1))
plot(rs, fval, type='l', xlab='r', ylab='f')
abline(h=0, lty=2)
plot(rs, gval, type='l', xlab='r', ylab='g')
abline(h=0, lty=2)
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 2/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 2/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 2/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 1/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=12, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 2/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 2/3
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
xi
optimize(objFun, c(-2,2), tau=TAU, y=y)           # optimize an objective function of L1 regression
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 regression
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
# 0. initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-02')
rm(list = ls())
# (2) load packages
source('Sub-02.R')                 # our own functions
# 1. solve a nonlinear equation
# (1) define a loss function
f <- function(r, p, Cs){
n <- length(Cs)
tt <- 1:n
loss <- p - sum(Cs/((1+r)^tt))
loss
}
# (2) find the solution
Cs <- c(2000, 2000, 2500, 4000)
P <- 7704
uniroot(f, c(0,1), p=P, Cs=Cs)           # find the zero root of f function
# 2. optimize a nonlinear function
# (1) optimize
g <- function(r, p, Cs) {f(r, p, Cs)^2}   # define g function: g=f^2
optimize(g, c(0,1), p=P, Cs=Cs)           # optimize g function
# (2) compare tow approaches
rs <- seq(0, 1, length=100)
fval <- gval <- numeric(length(rs))
for (i in seq_along(rs)){
fval[i] <- f(r=rs[i], p=P, Cs=Cs)
gval[i] <- g(r=rs[i], p=P, Cs=Cs)
}
par(mfrow=c(2,1))
plot(rs, fval, type='l', xlab='r', ylab='f')
abline(h=0, lty=2)
plot(rs, gval, type='l', xlab='r', ylab='g')
abline(h=0, lty=2)
par(mfrow=c(1,1))
# 3. optimize and L1 regression
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))
(xi_est <- xi[which.min(R)])
(xi_qua <- quantile(y, prob=TAU))
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 regression
# 3. compare nonlinear optimize and L1 statistics
# (1) do L1 statistics
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))                 # calculate objective function with different xi's
(xi_est <- xi[which.min(R)])                                # estimate xi which minimize objective function
(xi_qua <- quantile(y, prob=TAU))                           # quantile of observations which is the same as minimization point
# (2) optimize the nonlinear objective function
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 statistics
# 3. compare nonlinear optimize and L1 statistics
# (1) do L1 statistics
set.seed(12345)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))                 # calculate objective function with different xi's
(xi_est <- xi[which.min(R)])                                # estimate xi which minimize objective function
(xi_qua <- quantile(y, prob=TAU))                           # quantile of observations which is the same as minimization point
# (2) optimize the nonlinear objective function
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 statistics
# (1) do L1 statistics
set.seed(123)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))                 # calculate objective function with different xi's
(xi_est <- xi[which.min(R)])                                # estimate xi which minimize objective function
(xi_qua <- quantile(y, prob=TAU))                           # quantile of observations which is the same as minimization point
# (2) optimize the nonlinear objective function
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 statistics
# 3. compare nonlinear optimize and L1 statistics
# (1) do L1 statistics
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))                 # calculate objective function with different xi's
(xi_est <- xi[which.min(R)])                                # estimate xi which minimize objective function
(xi_qua <- quantile(y, prob=TAU))                           # quantile of observations which is the same as minimization point
# (2) optimize the nonlinear objective function
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 statistics
# 3. compare nonlinear optimize and L1 statistics
# (1) do L1 statistics
set.seed(1)
y <- rnorm(n=10, mean=0, sd=1)
xi <- seq(min(y), max(y), length=100)
TAU <- 0.5
(R <- objFun(tau=TAU, y, xi, plot.it=TRUE))                 # calculate objective function with different xi's
(xi_est <- xi[which.min(R)])                                # estimate xi which minimize objective function
(xi_qua <- quantile(y, prob=TAU))                           # quantile of observations which is the same as minimization point
# (2) optimize the nonlinear objective function
optimize(objFun, c(-2,2), tau=TAU, y=y, plot.it=FALSE)           # optimize an objective function of L1 statistics
source('Sub-02.R')                 # our own functions
y
sort(y)
?runif
optimize
?optimize
?optim
# 4. compare quadratic programming and OLS
# (1) generate data
beta <- c(5, 2)
sigma <- 1
n <- 100
set.seed(1)
eps <- rnorm(n, mean=0, sd=1)
x <- runif(n, min=-10, max=10)
y <- beta[1] + beta[2]*x + sigma*eps
# (2) do linear regression by OLS
model.lm <- lm(y, x)
(coef.OLS <- coef(model.lm))
# (3) solve quadratic programming by optim function
lossQuad <- function(betaHat, x, y){
sum((y-betaHat[1]-betaHat[2]*x)^2)
}
optim(par=coef.OLS, fn=lossQuad)           # optimize a loss function
y
x
# 4. compare quadratic programming and OLS
# (1) generate data
beta <- c(5, 2)
sigma <- 1
n <- 100
set.seed(1)
eps <- rnorm(n, mean=0, sd=1)
x <- runif(n, min=-10, max=10)
y <- beta[1] + beta[2]*x + sigma*eps
# (2) do linear regression by OLS
model.lm <- lm(y~x)
(coef.OLS <- coef(model.lm))
# (3) solve quadratic programming by optim function
lossQuad <- function(betaHat, x, y){
sum((y-betaHat[1]-betaHat[2]*x)^2)
}
optim(par=coef.OLS, fn=lossQuad)           # optimize a loss function
# 4. compare quadratic programming and OLS
# (1) generate data
beta <- c(5, 2)
sigma <- 1
n <- 100
set.seed(1)
eps <- rnorm(n, mean=0, sd=1)
x <- runif(n, min=-10, max=10)
y <- beta[1] + beta[2]*x + sigma*eps
# (2) do linear regression by OLS
model.lm <- lm(y~x)
(coef.OLS <- coef(model.lm))
# (3) solve quadratic programming by optim function
lossQuad <- function(betaHat, x, y){
sum((y-betaHat[1]-betaHat[2]*x)^2)
}
optim(par=coef.OLS, fn=lossQuad, y=y, x=x)           # optimize a loss function
# 4. compare quadratic programming and OLS
# (1) generate data
beta <- c(5, 2)
sigma <- 1
n <- 100
set.seed(1)
eps <- rnorm(n, mean=0, sd=1)
x <- runif(n, min=-10, max=10)
y <- beta[1] + beta[2]*x + sigma*eps
# (2) do linear regression by OLS
model.lm <- lm(y~x)
(coef.OLS <- coef(model.lm))
# (3) solve quadratic programming by optim function
lossQuad <- function(betaHat, x, y){
sum((y-betaHat[1]-betaHat[2]*x)^2)
}
optim(par=coef.OLS, fn=lossQuad, y=y, x=x)           # optimize a loss function
