library(ctm)
library(ctm)
library(ctm)
help(package='ctm')
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
# 0. 初始化
setwd('J:/programe/book/R with application to financial quantitive analysis/CH-05')
rm(list=ls())
# 1. 加载包
library(fGarch)
# 2. 模拟一元ARCH时间序列模型
# (1) simulate ARCH(1) model
set.seed(12345)
spec_1 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0))
simdata_1 <- garchSim(spec_1, n=200, extended=TRUE)
class(simdata_1)
plot(simdata_1)
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) residual series')
acf(simdata_1$garch, main='(b) simulate data')
acf(simdata_1$garch^2, main='(c) squared simulate data')
sumsq <- function(x){
y <- sum(x^2)
y
}
xx <- 1:5
sumsq(xx)
?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
names(lm.D9)
sumsq <- function(x){
y <- sum(x^2)
y
}
res <- function(y, x){
model.lm <- lm(y~x)
res <- model.lm$residuals
res
}
Q <- function(y, x){
ress <- res(y,x)
Q <- sumsq(ress)
Q
}
x <- seq(-5, 5, length=100)
y <- 5+2*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
x <- seq(-5, 5, length=100)
y <- 10+3*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
library(PerformanceAnalytics)
?CAPM.jensenAlpha
install.packages('fOption')
help(package='lmtest')
help(package='vars')
?read
?read.table
?save
?write
library(xlsx)
install.packages('xlsx')
library(xlsx)
library(xlsx)
library(xlsx)
library(ROCR)                     # for ROCR curve
install.packages('ROCR')
library(ROCR)                     # for ROCR curve
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
install.packages('RSNNS')
library(RSNNS)
library(RSNNS)
install.packages('Rcpp')
library(RSNNS)
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
dataValues= data[,1:7]
dataTargets = decodeClassLabels(data[,8])
data=splitForTrainingAndTest(dataValues, dataTargets, ratio=0.30)
data=normTrainingAndTestSet(data)
#建立rbf神经网络，并且训练
model2=rbf(data$inputsTrain,data$targetsTrain)
#预测
predictions2 = predict(model2,data$inputsTest)
#结果展示
confusionMatrix(data$targetsTest,predictions2)
?rbf
data
x <- as.matrix(data[,1:7])
dim(data)
data=read.delim("clipboard")
dat <- data
x <- as.matrix(dat[,1:7])
y <- as.matrix(dat[,8])
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
x
y
dim(x)
dim(y)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000,
initFuncParams=c(0, 1, 0, 0.01, 0.01),
learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)
inputs
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, size=3, maxit=1000)
model <- rbf(inputs, outputs, size=40, maxit=1000)
dim(x)
dim(y)
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y)
y
y
x
as.matrix(sin(inputs) + runif(inputs*0.2))
normalizeData(outputs, "0_1")
model.RBF <- rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
normalizeData(y, "0_1")
rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
inputs
outputs
outputs <- normalizeData(outputs, "0_1")
outputs
model <- rbf(inputs, outputs, size=40, maxit=1000)
inputs=x
outputs=normalizeData(y, "0_1")
inputs
class(inputs)
colnames(inputs)
colnames(inputs)<-''
colnames(inputs)<-NULL
head(inputs)
rbf(inputs, outputs)
rbf(inputs, outputs, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000)
rbf(inputs, outputs, size=40, maxit=1000)
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=3, maxit=1000)
model.RBF
par(mfrow=c(2,1))
plotIterativeError(model.RBF)
plot(inputs, outputs)
lines(inputs, fitted(model.RBF), col="green")
plotIterativeError(model.RBF)               # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
install.packages('DAAG')
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
library(DAAG)
install.packages('DAAG')
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
par(mfrow=c(1,1))
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=5, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=15, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
library(LASSO)
instll.packages('LASSO')
install.packages('LASSO')
library(lasso)
install.packages(lasso)
library(LARS)
install.packages('LARS')
install.packages('lasso')
install.packages('lars')
library(elasticnet)
install.packages('elasticnet')
install.packages('ncvreg')
help(package='ncvreg')
help(package='elasticnet')
help(package='lars')
help(package='lars')
help(package='elasticnet')
help(ncvreg)
help(package='ncvreg')
library(urca)
help(package='urca')
?cajolst
library(quantreg)
data(engel)
getwd()
write.csv(engel, file='ls.csv')
names(engel)
model.rq <- rq(foodexp~income, data=engel)
model.rq
?rq
taus <- seq(0.1,0.9, by=0.1)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
model.rq
taus <- seq(0.01,0.99, by=0.01)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
mid.income <- mean(engel$income)
mid.income
plot(engel$income)
?predict
foodfit <- predict(model.rq, newdata=data.frame(income=mid.income))
foodfit
plot(density(foodfit))
low.income <- quantile(engel$income, prob=0.25)
plot(income, foodexp, data=engel)
plot(engel$income, engel$foodexp)
??return
??returns
help(package='fPortfolio')
library(fPortfolio)
?returns
library(fPortfolioBacktest)
help(package='fPortfolioBacktest')
# 0. Initializing
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-10')
rm(list=ls())
# 0. initialize
# (1) set work path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-04')
rm(list=ls())
# (2) source packages
library(timeSeries)                                # for time series
library(tseries)                                   # for time series
library(RODBC)                                     # for data connect
library(quantmod)
# 1. process time series data
# (1) read and merge data
stock.names <- c('AAPL','GOOG')
getSymbols(stock.names, from='2014-03-01',to='2014-05-31')   # get two assets at the same time
start(AAPL)
end(AAPL)
class(AAPL)
dim(AAPL)                                            # obs of AAPLE is bigger than that of GOOGLE
dim(GOOG)                                            # less than that of AAPL
assets <- merge.xts(AAPL, GOOG, all=FALSE)
dim(assets)
# (2) subset data
assets <- assets[,c('AAPL.Close', 'GOOG.Close')]     # select columns
assets['2014-05']                                    # select rows
assets[start(assets),]                               # find the first record
assets[end(assets),]                                 # find the last record
subset(assets, (assets[,1]>600) & (assets[,2]<550))  # use subset function
# (3) sample randomly and sort data
assets.ts <- as.timeSeries(assets)                 # change class of obj
class(assets.ts)
assets.samp <- sample(assets.ts, size=40)
dim(assets.samp)
print(assets.samp)
sort(assets.samp)
# (4) align data
assets.ali <- align(assets.samp, by='1d', method='before', include.weekends=FALSE)
dim(assets.ali)
print(assets.ali)
# (5) change frequency
assets.m <- to.monthly(assets)
print(assets.m)
# (6) roll data
rollapply <- function(x, by, FUN, ...){
ans <- applySeries(x, from=by$from, to=by$to, by=NULL, FUN=FUN, format=x@format,
zone=finCenter(x), FinCenter=finCenter(x), title=x@title, documentation=x@documentation, ...)
attr(ans, 'by') <- data.frame(from=format(by$from), to=format(by$to))
ans
}
rts <- diff(log(assets))
rts <- returns(assets, method='continuous', percentage=TRUE)
rts <- as.timeSeries(na.omit(rts))
by <- periods(time(rts), period='1m', by='1d')
rts.roll <- rollapply(rts, by=by, FUN='colSums')
print(rts.roll)
# 2. process cross sectional data
# (1) reading data from EXCEL
Z <- odbcConnectExcel2007('FirmData.xlsx')                                # run faster
Data_Qvalue <- sqlFetch(Z, 'Qvalue', max=65536)
Data_Industry <- sqlFetch(Z, 'Industry', max=2717)
Data_Concent <- sqlFetch(Z, 'Concent', max=43227)
Data_Ownership <- sqlFetch(Z, 'Ownership', max=40899)
Data_Finance <- sqlFetch(Z, 'Finance', max=65532)
Data_Banlance <- sqlFetch(Z, 'Banlance', max=115619)
close(Z)
detach(package:RODBC)
tail(Data_Industry)
tail(Data_Concent)
tail(Data_Ownership)
tail(Data_Finance)
tail(Data_Banlance)
# (2) add names of firms
firmName <- function(name){
name.rev <- paste('000000', as.character(name), sep='')
name.rev <- substr(name.rev, start=nchar(name.rev)-5, stop=nchar(name.rev))
name.c <- paste('c', name.rev, sep='')
name.c
}
Data_Qvalue$Comcd <- firmName(Data_Qvalue$Stkcd)
Data_Industry$Comcd <- firmName(Data_Industry$Stkcd)
Data_Concent$Comcd <- firmName(Data_Concent$Stkcd)
Data_Ownership$Comcd <- firmName(Data_Ownership$Stkcd)
Data_Finance$Comcd <- firmName(Data_Finance$Stkcd)
Data_Banlance$Comcd <- firmName(Data_Banlance$Stkcd)
# (3) extract industry information in year 2012
# identify financial firm by '1'
Data_Industry <- Data_Industry[!is.na(Data_Industry$Indcd),]
NonfinComcd <- levels(factor(as.character(Data_Industry[Data_Industry$Indcd != 1, 'Comcd'])))
# keep non financial firm
Qvalue.Nonfin <- Data_Qvalue[(Data_Qvalue$Comcd %in% NonfinComcd), ]
Industry.Nonfin <- Data_Industry[(Data_Industry$Comcd %in% NonfinComcd), ]
Concent.Nonfin <- Data_Concent[(Data_Concent$Comcd %in% NonfinComcd), ]
Ownership.Nonfin <- Data_Ownership[(Data_Ownership$Comcd %in% NonfinComcd), ]
Finance.Nonfin <- Data_Finance[(Data_Finance$Comcd %in% NonfinComcd), ]
Banlance.Nonfin <- Data_Banlance[(Data_Banlance$Comcd %in% NonfinComcd), ]
# (4) select rows and cols
names.Qvalue <- c('Comcd', 'Accper', 'QVal')
names.Industry <- c('Comcd', 'Indcd')
names.Concent <- c('Comcd', 'Reptdt', 'OwnCon1', 'OwnCon5', 'OwnCon10')
names.Ownership <- c('Comcd', 'Reptdt', 'Indicator', 'State')
names.Finance <- c('Comcd', 'Accper', 'ROA', 'ROE')
names.Banlance <- c('Comcd', 'Accper', 'CurrentAsset', 'TotalAsset', 'CurrentLib', 'TotalLib')
Qvalue.tab <- Qvalue.Nonfin[substr(Qvalue.Nonfin$Accper, start=6, stop=7)=='12', names.Qvalue]
Industry.tab <- Industry.Nonfin[,names.Industry]
Concent.tab <- Concent.Nonfin[substr(Concent.Nonfin$Reptdt, start=6, stop=7)=='12', names.Concent]
Finance.tab <- Finance.Nonfin[substr(Finance.Nonfin$Accper, start=6, stop=7)=='12', names.Finance]
Banlance.tab <- Banlance.Nonfin[substr(Banlance.Nonfin$Accper, start=6, stop=7)=='12', names.Banlance]
Ownership.tab <- Ownership.Nonfin[Ownership.Nonfin$Indicator==1, names.Ownership]
Ownership.tab$state <- 0
Ownership.tab$state[(Ownership.tab$State == 1100) | (Ownership.tab$State == 2100)] <- 1
# (5) compute variables
Qvalue.tab$Enddt <- as.numeric(substr(Qvalue.tab$Accper, 1, 4))
Concent.tab$Enddt <- as.numeric(substr(Concent.tab$Reptdt, 1, 4))
Ownership.tab$Enddt <- as.numeric(substr(Ownership.tab$Reptdt, 1, 4))
Finance.tab$Enddt <- as.numeric(substr(Finance.tab$Accper, 1, 4))
Banlance.tab$Enddt <- as.numeric(substr(Banlance.tab$Accper, 1, 4))
Concent.tab$EBD <- Concent.tab$OwnCon5/Concent.tab$OwnCon1
Banlance.tab$Size <- log(Banlance.tab$TotalAsset)
Banlance.tab$Currt <- Banlance.tab$CurrentAsset/Banlance.tab$CurrentLib
Banlance.tab$AssLibRatio <- Banlance.tab$TotalLib/Banlance.tab$TotalAsset
# (6) merge data
KeyField1 <- 'Comcd'         # key varialbes for matching
KeyField2 <- 'Enddt'         # key varialbes for matching
Qvalue.Finance <- merge(Qvalue.tab, Finance.tab, by=c(KeyField1, KeyField2))
Qvalue.Finance.Concent <- merge(Qvalue.Finance, Concent.tab, by=c(KeyField1, KeyField2))
Qvalue.Finance.Concent.Ownership <- merge(Qvalue.Finance.Concent, Ownership.tab, by=c(KeyField1, KeyField2))
Qvalue.Finance.Concent.Ownership.Banlance <- merge(Qvalue.Finance.Concent.Ownership, Banlance.tab, by=c(KeyField1, KeyField2))
Qvalue.Finance.Concent.Ownership.Banlance.Industry <- merge(Qvalue.Finance.Concent.Ownership.Banlance, Industry.tab,
by=c(KeyField1))
# (7) remove NA values
var.names <- c("Comcd", "Enddt", "QVal", "ROA", "ROE", "OwnCon1", "OwnCon10", "EBD", "state","Size",
"CurrentAsset", "TotalAsset", "CurrentLib", "TotalLib", "Currt", "AssLibRatio", "Indcd")
Data <- Qvalue.Finance.Concent.Ownership.Banlance.Industry[ ,var.names]
Data <- na.omit(Data)
# (8) set dummy variables
Data$Year02 <- 0
Data$Year03 <- 0
Data$Year04 <- 0
Data$Year05 <- 0
Data$Year06 <- 0
Data$Year07 <- 0
Data$Year08 <- 0
Data$Year09 <- 0
Data$Year10 <- 0
Data$Year11 <- 0
Data$Year12 <- 0
Data$Year13 <- 0
Data[Data$Enddt == 2004, 'Year04'] <- 1
Data[Data$Enddt == 2005, 'Year05'] <- 1
Data[Data$Enddt == 2006, 'Year06'] <- 1
Data[Data$Enddt == 2007, 'Year07'] <- 1
Data[Data$Enddt == 2008, 'Year08'] <- 1
Data[Data$Enddt == 2009, 'Year09'] <- 1
Data[Data$Enddt == 2010, 'Year10'] <- 1
Data[Data$Enddt == 2011, 'Year11'] <- 1
Data[Data$Enddt == 2012, 'Year12'] <- 1
Data[Data$Enddt == 2013, 'Year13'] <- 1
dim(Data)
head(Data)
head(Data)
write.csv(Data, file='ls.csv')
#########################################################
# 0. initialize
# (1) set work path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-05')
rm(list=ls())
# (2) source packages
library(zoo)                    # for time series
library(timeSeries)             # for portfolio
# 1. calculate individual stock returns
# (1) read data from txt file
da <- read.table("5-1.txt",head=T,colClasses="character")
clsprc <- as.numeric(da[,3])
dates <- as.Date(da[,2])
cls <- zoo(clsprc, dates)                   # change object's class
lclsprc <- lag(cls,k=-1)                    # set one lag
clsprc <- as.numeric(da[2:237,3])
lclsprc <- as.numeric(lclsprc)
# (2) calculate daily returns use three approaches
re <- 100 * (log(clsprc)-log(lclsprc))                        # use formula
re.1 <- 100 * diff(log(cls))                                  # use function
re.2 <- returns(cls, method='continuous', percentage=TRUE)    # use function
# (3) calculate arithmetic mean
(am <- mean(re))                                              # the same thing
(am.1 <- mean(re.1))                                          # the same thing
(am.2 <- mean(re.2, na.rm=TRUE))                              # the same thing
identical(re, re.1)
identical(round(re, digits=4), round(re.1, digits=4))
re
re.1
identical(re, as.numeric(re.1))
identical(re, as.numeric(re.2))
re.2
identical(re, as.numeric(na.omit(re.2)))
class(re)
class(re.1)
# 2. calculate multi-stock returns
# (1) read data
da1 <- read.table("CAQC.txt",head=T,colClasses="character")
da2 <- read.table("ZXTX.txt",head=T,colClasses="character")
wcls1 <- as.numeric(da1[,3])
wcls2 <- as.numeric(da2[,3])
dates1 <- as.yearmon(da1[,2])
dates2 <- as.yearmon(da2[,2])
cls1 <- zoo(wcls1,dates1)
cls2 <- zoo(wcls2,dates2)
dat.merge <- merge(cls1, cls2)                                # merge data by date
# (2) calculate returns
r.merge <- returns(dat.merge, method='continuous', percentage=TRUE)
print(r.merge)
