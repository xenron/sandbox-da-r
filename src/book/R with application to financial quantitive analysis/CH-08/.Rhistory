if( length(check.points) == 1 )
check.points = seq(max(min(x),1e-100), max(x), length=check.points)
np = length(check.points)
if( missing(L) ) L = dpoismix(mix, x)
dg = colSums(w * outer(x, check.points, dpois) *
(outer(x, check.points, "/")-1) / L)
jmax = (1:(np-1))[dg[1:(np-1)] > 0 & dg[2:np] < 0]
g1 = colSums(w * outer(x, check.points[1], dpois) / L) - sum( w )
if( length(jmax) < 1 ) {
if(dg[1] < 0 && g1 > tol )
return( list(lambda=check.points[1], gradient=g1) )
else return
}
lambda = (check.points[jmax] + check.points[jmax+1])/2
repeat {
lambda.old = lambda
lambda = lambda -
colSums(w * outer(x, lambda, dpois) * (outer(x, lambda, "/")-1) / L) /
colSums(w * outer(x, lambda, dpois) *
((outer(x, lambda, "/")-1)^2 - outer(x, lambda^2, "/")) / L)
lambda[lambda<check.points[1]] = check.points[1]
if( max(abs(lambda - lambda.old)) < 1e-6 ) break
}
g = colSums(w * outer(x, lambda, dpois) / L) - sum(w)
j = g >= tol
g1 = colSums(w * outer(x, check.points[1], dpois) / L) - sum( w )
if(dg[1] < 0 && g1 > 0 )
list(lambda=c(check.points[1],lambda[j]), gradient=c(g1,g[j]))
else list(lambda=lambda[j], gradient=g[j])
}
# --------------------------------- #
# Mixtures of Poisson Distributions #
# --------------------------------- #
# Creates a "poismix" object
poismix = function(lambda=NULL, pi=1/length(lambda)) {
if( is.null(lambda) ) a = list(lambda=NULL, pi=NULL)  # NULL mixture
else {               # If lambda == NA, one component with unknown mean
k = max(length(lambda), length(pi), na.rm=TRUE)
lambda = rep(lambda, len=k)
pi = rep(pi, len=k)
a = list(lambda=lambda, pi=pi/sum(pi))
}
class(a) = "poismix"
a
}
is.null.poismix = function(a) is.null(a$lambda)
print.poismix = function(a) {
if( is.null(a$lambda) ) b = matrix(nrow=0, ncol=2)
else b = cbind(a$lambda, a$pi)
colnames(b) = c("lambda","pi")
print(b)
}
sort.poismix = function(a) {
if( is.null(a) ) return(a)
i = order(a$lambda)
a$lambda = a$lambda[i]
a$pi = a$pi[i]
a
}
is.unsorted.poismix = function(a) is.unsorted(a$lambda)
# Unique
unique.poismix = function(a, precision=c(0.01,1e-6)) {
if( length(a$lambda) == 1 ) return(a)
if( is.unsorted.poismix(a) ) a = sort.poismix(a)
if ( max(precision) == 0 ) return(a)
precision = rep(precision, len=3)
count = i1 = i2 = 1
for( i in 2:length(a$lambda) ) {
if( a$lambda[i] - a$lambda[count] > precision[1] &&
min(max(a$pi[count:(i-1)]), a$pi[i]) > precision[2] ) {
a$lambda[count] = weighted.mean(a$lambda[i1:i2], a$pi[i1:i2])
a$pi[count] = sum(a$pi[i1:i2])
count = count + 1
i1 = i2 = i
a$lambda[count] = a$lambda[i]
}
else i2 = i
}
a$lambda[count] = weighted.mean(a$lambda[i1:i2], a$pi[i1:i2])
a$pi[count] = sum(a$pi[i1:i2])
poismix(lambda=a$lambda[1:count], pi=a$pi[1:count])
}
# Approximately equal
aeq.poismix = function(a, b, precision=c(1e-6,1e-6)) {
if( is.null(a) && is.null(b) ) return( TRUE )
if( is.null(a) || is.null(b) ) return( FALSE )
if( length(a$lambda) != length(b$lambda) ) return( FALSE )
if( max(abs(a$lambda - b$lambda) / sqrt(pmin(a$lambda, b$lambda)+1e-20))
>= precision[1] )
return( FALSE )
if( max(abs(a$pi - b$pi)) >= precision[2]) return( FALSE )
TRUE
}
# Returns a random sample of a Poisson mixture
# n        Sample size
# lambda   Means
# pi       Proportions for all components
# fixed    = TRUE, proportion of observations from each component is fixed
rpoismix = function(n=50, lambda=c(1,4), pi=1, mix, fixed=TRUE) {
if( ! missing(mix) ) {lambda =  mix$lambda; pi=mix$pi}
if( n == 0 ) return( numeric(0) )
k = length(lambda)
pi = rep(pi, len=k)
pi = pi / sum(pi)
if( fixed ) {
nj = floor(n * pi)
x = rpoismix(n - sum(nj), lambda=lambda, pi=pi, fixed=FALSE)
}
else {
cs = colSums(outer(runif(n), cumsum(pi), "<"))
nj = c(cs[1], diff(cs))
x = numeric(0)
}
for( j in 1:k ) x = c(x, rpois(nj[j], lambda[j]))
sample(x)
}
# ---------------------- #
# Distribution functions #
# ---------------------- #
# Density function of a Poisson mixture
dpoismix = function(a, x, log=FALSE) {
log.dpois = function(x, lambda) dpois(x, lambda, log=TRUE)
if (log) {
logd = outer(x, a$lambda, log.dpois)
ma = apply(logd, 1, max)
ma + log(rowSums(sweep(exp(sweep(logd, 1, ma, "-")), 2, a$pi, "*")))
}
else rowSums(sweep(outer(x, a$lambda, dpois), 2, a$pi, "*"))
}
# log-likelihood
logLik.poismix = function( a, x, w=1 ) sum( w * dpoismix(a, x, log=TRUE) )
# Line search
# mix1      Current poismix object
# mix2      New poismix object
# x         Observations
# w         Frequency
# ll.mix1   log-likelihood of mix1
# lsm       Line search method, either step-halving (halving) or optimal (optim)
line.poismix = function(mix1, mix2, x, w, ll.mix1=NULL,
lsm=c("halving","optim")) {
lsm = match.arg(lsm)
ll.alpha = function(alpha) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
logLik(m, x, w)
}
if( is.null(ll.mix1) ) ll.mix1 = logLik(mix2, x, w)
convergence = 0
alpha = 1
repeat {
if( lsm == "optim" ) {
m = poismix( (1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi )
d = m$pi - mix1$pi
gll = sum(gradient.poismix(m, x, w)$gradient * d)
if( gll < 0 ) alpha = optimize(ll.alpha, lower=0, upper=alpha, max=TRUE,
tol=1e-3)$maximum
}
ll.mix = ll.alpha(alpha)
if( ll.mix > ll.mix1 ) break
if( alpha < 1e-10 ) {convergence = 1; break}
alpha = alpha / 2
}
list(mix = poismix((1-alpha) * mix1$lambda + alpha * mix2$lambda,
(1-alpha) * mix1$pi + alpha * mix2$pi),
ll.mix=ll.mix, alpha=alpha, convergence=convergence)
}
# Gradient of the log-likelihood
gradient.poismix = function(mix, x, w, individual=FALSE,
hessian=FALSE) {
dij = outer(x, mix$lambda, dpois)
di = as.vector(dij %*% mix$pi)
gradient.i = w * dij / di
if( individual ) list( gradient = gradient.i )
else list( gradient = colSums(gradient.i) )
}
mix.efron = normmix(mu=c(-10.9, -7.0, -4.9, -1.8, -1.1, 0.0, 2.4, 6.1),
pi=c(1.5, 1.3, 5.6, 12.3, 13.6, 60.8, 2.7, 2.2))
set.seed(1)
x = rnormmix(n=1000, mix=mix.efron)
cnm.normmix(x, tol=1e-5)
thai = data.frame( x=c(0:21,23,24),
freq=c(120,64,69,72,54,35,36,25,25,19,18,18,13,4,3,6,6,5,1,3,1,2,1,2) )
cnm.poismix(x=thai$x, w=thai$freq)
nnls
library(quantreg)
?rq
rq
rq.fit
rq.fit.fnb
cnm.normmix(x, tol=1e-5)
cnm.poismix(x=thai$x, w=thai$freq)
install.packages('ctm')
install.packages("ctm", repos="http://R-Forge.R-project.org")
library(ctm)
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
library(ctm)
library(ctm)
library(ctm)
help(package='ctm')
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
install.packages("ctmDevel", repos="http://R-Forge.R-project.org")
# 0. 初始化
setwd('J:/programe/book/R with application to financial quantitive analysis/CH-05')
rm(list=ls())
# 1. 加载包
library(fGarch)
# 2. 模拟一元ARCH时间序列模型
# (1) simulate ARCH(1) model
set.seed(12345)
spec_1 <- garchSpec(model=list(omega=0.01, alpha=0.85, beta=0))
simdata_1 <- garchSim(spec_1, n=200, extended=TRUE)
class(simdata_1)
plot(simdata_1)
par(mfrow=c(1,3))
acf(simdata_1$eps, main='(a) residual series')
acf(simdata_1$garch, main='(b) simulate data')
acf(simdata_1$garch^2, main='(c) squared simulate data')
sumsq <- function(x){
y <- sum(x^2)
y
}
xx <- 1:5
sumsq(xx)
?lm
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
names(lm.D9)
sumsq <- function(x){
y <- sum(x^2)
y
}
res <- function(y, x){
model.lm <- lm(y~x)
res <- model.lm$residuals
res
}
Q <- function(y, x){
ress <- res(y,x)
Q <- sumsq(ress)
Q
}
x <- seq(-5, 5, length=100)
y <- 5+2*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
x <- seq(-5, 5, length=100)
y <- 10+3*x+rnorm(100)
Qstat <- Q(y, x)
Qstat
library(PerformanceAnalytics)
?CAPM.jensenAlpha
install.packages('fOption')
help(package='lmtest')
help(package='vars')
?read
?read.table
?save
?write
library(xlsx)
install.packages('xlsx')
library(xlsx)
library(xlsx)
library(xlsx)
library(ROCR)                     # for ROCR curve
install.packages('ROCR')
library(ROCR)                     # for ROCR curve
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
install.packages('RSNNS')
library(RSNNS)
library(RSNNS)
install.packages('Rcpp')
library(RSNNS)
data=read.delim("clipboard")
data <- dat
data= data[sample(1:nrow(data),length(1:nrow(data))),1:ncol(data)]
dataValues= data[,1:7]
dataTargets = decodeClassLabels(data[,8])
data=splitForTrainingAndTest(dataValues, dataTargets, ratio=0.30)
data=normTrainingAndTestSet(data)
#建立rbf神经网络，并且训练
model2=rbf(data$inputsTrain,data$targetsTrain)
#预测
predictions2 = predict(model2,data$inputsTest)
#结果展示
confusionMatrix(data$targetsTest,predictions2)
?rbf
data
x <- as.matrix(data[,1:7])
dim(data)
data=read.delim("clipboard")
dat <- data
x <- as.matrix(dat[,1:7])
y <- as.matrix(dat[,8])
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
x
y
dim(x)
dim(y)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000,
initFuncParams=c(0, 1, 0, 0.01, 0.01),
learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)
inputs
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y, size=40, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, maxit=1000)
model.RBF <- rbf(inputs=x, outputs=y, size=3, maxit=1000)
model <- rbf(inputs, outputs, size=40, maxit=1000)
dim(x)
dim(y)
dim(inputs)
dim(outputs)
model.RBF <- rbf(inputs=x, outputs=y)
y
y
x
as.matrix(sin(inputs) + runif(inputs*0.2))
normalizeData(outputs, "0_1")
model.RBF <- rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
normalizeData(y, "0_1")
rbf(inputs=x, outputs=normalizeData(y, "0_1"), size=3, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
inputs
outputs
outputs <- normalizeData(outputs, "0_1")
outputs
model <- rbf(inputs, outputs, size=40, maxit=1000)
inputs=x
outputs=normalizeData(y, "0_1")
inputs
class(inputs)
colnames(inputs)
colnames(inputs)<-''
colnames(inputs)<-NULL
head(inputs)
rbf(inputs, outputs)
rbf(inputs, outputs, maxit=1000)
inputs <- as.matrix(seq(0,10,0.1))
outputs <- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs <- normalizeData(outputs, "0_1")
model <- rbf(inputs, outputs, size=40, maxit=1000)
rbf(inputs, outputs, size=40, maxit=1000)
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=3, maxit=1000)
model.RBF
par(mfrow=c(2,1))
plotIterativeError(model.RBF)
plot(inputs, outputs)
lines(inputs, fitted(model.RBF), col="green")
plotIterativeError(model.RBF)               # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
install.packages('DAAG')
DAAG::confusion(round(y.hat), y)             # calculate confusion matrix
library(DAAG)
install.packages('DAAG')
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
par(mfrow=c(1,1))
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=5, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
dataTargets <- decodeClassLabels(y)
model.RBF <- rbf(inputs, outputs, size=15, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
# 4. make RBF neural network
# (1) estimate model
set.seed(12345)
inputs <- x
colnames(inputs) <- NULL
outputs <- normalizeData(y, "0_1")
model.RBF <- rbf(inputs, outputs, size=10, maxit=1000)
# (2) show results
plotIterativeError(model.RBF)                              # show iterations
y.hat <- predict(model.RBF, inputs)
DAAG::confusion(round(y.hat), y)                           # calculate confusion matrix
plot(performance(prediction(y.hat, y), "tpr","fpr"))       # plot ROC
performance(prediction(y.hat, y), "auc")@y.values[[1]]     # accuracy
library(LASSO)
instll.packages('LASSO')
install.packages('LASSO')
library(lasso)
install.packages(lasso)
library(LARS)
install.packages('LARS')
install.packages('lasso')
install.packages('lars')
library(elasticnet)
install.packages('elasticnet')
install.packages('ncvreg')
help(package='ncvreg')
help(package='elasticnet')
help(package='lars')
help(package='lars')
help(package='elasticnet')
help(ncvreg)
help(package='ncvreg')
library(urca)
help(package='urca')
?cajolst
library(quantreg)
data(engel)
getwd()
write.csv(engel, file='ls.csv')
names(engel)
model.rq <- rq(foodexp~income, data=engel)
model.rq
?rq
taus <- seq(0.1,0.9, by=0.1)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
model.rq
taus <- seq(0.01,0.99, by=0.01)
model.rq <- rq(foodexp~income, tau=taus,data=engel)
mid.income <- mean(engel$income)
mid.income
plot(engel$income)
?predict
foodfit <- predict(model.rq, newdata=data.frame(income=mid.income))
foodfit
plot(density(foodfit))
low.income <- quantile(engel$income, prob=0.25)
plot(income, foodexp, data=engel)
plot(engel$income, engel$foodexp)
??return
??returns
help(package='fPortfolio')
library(fPortfolio)
?returns
library(fPortfolioBacktest)
help(package='fPortfolioBacktest')
# 0. initialize
# (1) set path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-08')
rm(list=ls())
options(digits=4, width=70)
# 0. initialize
# (1) set work path
setwd('F:/programe/book/R with application to financial quantitive analysis/CH-08')
rm(list=ls())
# (2) load package
library(MASS)
library(Rdonlp2)
library(lars)
library(msgps)
source('Sub-08.R')
